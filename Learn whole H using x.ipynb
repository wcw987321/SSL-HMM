{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define some useful functions\n",
    "def generate_HMM_params(num_hidden_state, num_obs):\n",
    "    # random generate the transition matrix and observation matrix, and compute the stationary distribution\n",
    "    \n",
    "    alpha_state = np.ones(num_hidden_state)\n",
    "    alpha_obs = np.ones(num_obs) / num_obs\n",
    "    trans_mat = np.random.dirichlet(alpha_state, num_hidden_state)\n",
    "    obs_mat = np.random.dirichlet(alpha_obs, num_hidden_state)\n",
    "    tmp = np.ones((num_hidden_state + 1, num_hidden_state))\n",
    "    tmp[:-1] = np.identity(num_hidden_state) - trans_mat.T\n",
    "    tmp_v = np.zeros(num_hidden_state + 1)\n",
    "    tmp_v[-1] = 1\n",
    "    stat_dist = np.linalg.lstsq(tmp, tmp_v, rcond=None)[0]\n",
    "    return trans_mat, obs_mat, stat_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_HMM_sequences(trans_mat, obs_mat, init_dist, length, num_samples = 1):\n",
    "    # generate sample sequences from HMM\n",
    "    \n",
    "    states = np.zeros((num_samples, length))\n",
    "    obs = np.zeros((num_samples, length))\n",
    "    tmp_state = np.argmax(np.random.multinomial(1, init_dist, num_samples), axis = 1)\n",
    "    #print(tmp_state)\n",
    "    for i in range(length):\n",
    "        #print(\"i: \", i)\n",
    "        states[:, i] = tmp_state\n",
    "        for j in range(num_samples):\n",
    "            obs[j, i] = np.random.multinomial(1, obs_mat[tmp_state[j]]).argmax()\n",
    "            tmp_state[j] = np.random.multinomial(1, trans_mat[tmp_state[j]]).argmax()\n",
    "        #print(\"obs[:, i]: \", obs[:, i])\n",
    "    return states, obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_compute(trans_mat, obs_mat, init_dist, obs_to_pos):\n",
    "    # compute \\sum_{h_1,...,h_{pos-1}} P(h_1,...,h_{pos},x_1,...,x_{pos-1})\n",
    "    pos = obs_to_pos.shape[0] + 1\n",
    "    num_hidden_state = trans_mat.shape[0]\n",
    "    num_obs = obs_mat.shape[1]\n",
    "    forward = np.zeros((pos, num_hidden_state))\n",
    "    forward[0] = init_dist\n",
    "    for i in range(1, pos):\n",
    "        for j in range(num_hidden_state):\n",
    "            for k in range(num_hidden_state):\n",
    "                #print(i, j, k)\n",
    "                #print(forward[i - 1, k], trans_mat[k, j], obs_mat[k, int(obs_to_pos[i - 1])])\n",
    "                forward[i, j] += forward[i - 1, k] * trans_mat[k, j] * obs_mat[k, int(obs_to_pos[i - 1])]\n",
    "    #print(\"forward: \", forward)\n",
    "    return forward[pos - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_compute(trans_mat, obs_mat, obs_from_pos):\n",
    "    num_hidden_state = trans_mat.shape[0]\n",
    "    num_obs = obs_mat.shape[1]\n",
    "    back_length = obs_from_pos.shape[0]\n",
    "    if (back_length == 0):\n",
    "        return np.ones(num_hidden_state)\n",
    "    backward = np.zeros((back_length, num_hidden_state))\n",
    "    for j in range(num_hidden_state):\n",
    "         for k in range(num_hidden_state):\n",
    "            backward[0, j] += trans_mat[j, k] * obs_mat[k, int(obs_from_pos[-1])]\n",
    "    for i in range(1, back_length):\n",
    "        for j in range(num_hidden_state):\n",
    "            for k in range(num_hidden_state):\n",
    "                backward[i, j] += trans_mat[j, k] * obs_mat[k, int(obs_from_pos[-(i + 1)])] * backward[i - 1, k]\n",
    "    #print(\"backward: \", backward)\n",
    "    return backward[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_i_conditional_prob(trans_mat, obs_mat, init_dist, known_X, pos):\n",
    "    num_hidden_state = trans_mat.shape[0]\n",
    "    num_obs = obs_mat.shape[1]\n",
    "    num_samples = known_X.shape[0]\n",
    "    length = known_X.shape[1]\n",
    "    x_pos_conditional_prob = np.zeros((num_samples, num_obs))\n",
    "    h_pos_conditional_prob = np.zeros((num_samples, num_hidden_state))\n",
    "    h_all_pos_conditional_prob = np.zeros((num_samples, num_hidden_state))\n",
    "    for i in range(num_samples):\n",
    "        #print(\"x_i_conditional_prob: i=\", i)\n",
    "        sample_obs_vec = known_X[i]\n",
    "        forward_vec = forward_compute(trans_mat, obs_mat, init_dist, known_X[i, :pos[i]])\n",
    "        backward_vec = backward_compute(trans_mat, obs_mat, known_X[i, pos[i] + 1:])\n",
    "        #print(\"forward_vec: \", forward_vec)\n",
    "        #print(\"backward_vec: \", backward_vec)\n",
    "        h_prob_tmp = forward_vec * backward_vec\n",
    "        tmp = h_prob_tmp.sum()\n",
    "        h_prob_tmp /= tmp\n",
    "        h_pos_conditional_prob[i] = h_prob_tmp\n",
    "        x_pos_conditional_prob[i] = h_prob_tmp @ obs_mat\n",
    "        h_all_pos_conditional_prob[i] = h_prob_tmp * obs_mat[:, int(known_X[i, pos[i]])] / x_pos_conditional_prob[i, int(known_X[i, pos[i]])]\n",
    "    return h_pos_conditional_prob, x_pos_conditional_prob, h_all_pos_conditional_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameter setting\n",
    "num_hidden_state = 3\n",
    "num_obs = 5\n",
    "length = 10\n",
    "num_samples = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transition matrix\n",
      "[[0.19038151 0.5603291  0.24928939]\n",
      " [0.59211519 0.37810341 0.0297814 ]\n",
      " [0.52737761 0.34205379 0.1305686 ]]\n",
      "observation matrix\n",
      "[[2.84422161e-01 2.51497236e-01 1.31816210e-02 4.38026768e-01\n",
      "  1.28722143e-02]\n",
      " [1.50940815e-01 6.59969385e-01 1.64551320e-02 2.15946003e-02\n",
      "  1.51040067e-01]\n",
      " [9.01408117e-03 9.45219115e-01 4.57668030e-02 7.87716140e-11\n",
      "  4.61558184e-10]]\n",
      "stationary distribution\n",
      "[0.41619456 0.44908825 0.1347172 ]\n",
      "states and observations, first half of each row is states, only showing first 5: \n",
      "[[1. 1. 0. 0. 0. 2. 0. 1. 0. 1. 0. 1. 0. 3. 3. 1. 1. 0. 3. 1.]\n",
      " [1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 4. 3. 3. 4. 1. 0. 4. 1. 1.]\n",
      " [0. 2. 0. 2. 0. 2. 2. 2. 1. 0. 3. 1. 1. 1. 0. 1. 1. 1. 1. 1.]\n",
      " [0. 2. 2. 0. 1. 0. 2. 1. 2. 0. 1. 1. 1. 3. 1. 0. 1. 1. 1. 2.]\n",
      " [1. 0. 1. 0. 2. 1. 1. 1. 0. 1. 4. 1. 4. 1. 1. 0. 1. 0. 3. 3.]]\n",
      "positions, only showing first 5:  [9 4 6 2 1]\n",
      "Pr[H_i|x_-i], j-th row is for j-th sample and i=positions[j], only showing first 5:\n",
      "[[0.21892801 0.54738046 0.23369153]\n",
      " [0.33619463 0.45004706 0.21375831]\n",
      " [0.60101586 0.31777197 0.08121217]\n",
      " [0.24157682 0.61921196 0.13921122]\n",
      " [0.62919903 0.33535031 0.03545066]]\n",
      "Pr[X_i|x_-i], j-th row is for j-th sample and i=positions[j], only showing first 5:\n",
      "[[0.14699655 0.63720383 0.02258836 0.10771679 0.08549447]\n",
      " [0.16547851 0.58361774 0.02162021 0.15698083 0.07230271]\n",
      " [0.21963904 0.43763689 0.01686816 0.2701232  0.0557327 ]\n",
      " [0.16342902 0.60100195 0.01974484 0.11918875 0.09663544]\n",
      " [0.22989575 0.4130714  0.01543456 0.28284777 0.05875052]]\n",
      "Pr[H_i|x_i,x_-i], j-th row is for j-th sample and i=positions[j], only showing first 5:\n",
      "[[8.64084416e-02 5.66936866e-01 3.46654692e-01]\n",
      " [5.98534890e-02 9.40146510e-01 1.36456714e-09]\n",
      " [3.45386395e-01 4.79209535e-01 1.75404070e-01]\n",
      " [1.01091024e-01 6.79966078e-01 2.18942898e-01]\n",
      " [3.83085874e-01 5.35793431e-01 8.11206949e-02]]\n"
     ]
    }
   ],
   "source": [
    "seed = 20211018\n",
    "np.random.seed(seed)\n",
    "trans_mat, obs_mat, stat_dist = generate_HMM_params(num_hidden_state, num_obs) # generate parameters for HMM\n",
    "\n",
    "states, obs = generate_HMM_sequences(trans_mat, obs_mat, stat_dist, length, num_samples) # generate sample sequences\n",
    "\n",
    "pos = np.random.randint(length, size = num_samples)\n",
    "\n",
    "print(\"transition matrix\")\n",
    "print(trans_mat)\n",
    "print(\"observation matrix\")\n",
    "print(obs_mat)\n",
    "print(\"stationary distribution\")\n",
    "print(stat_dist)\n",
    "print(\"states and observations, first half of each row is states, only showing first 5: \")\n",
    "print(np.concatenate((states, obs), axis = 1)[:5])\n",
    "print(\"positions, only showing first 5: \", pos[:5])\n",
    "h, x, hh = x_i_conditional_prob(trans_mat, obs_mat, stat_dist, obs, pos)\n",
    "print(\"Pr[H_i|x_-i], j-th row is for j-th sample and i=positions[j], only showing first 5:\")\n",
    "print(h[:5])\n",
    "print(\"Pr[X_i|x_-i], j-th row is for j-th sample and i=positions[j], only showing first 5:\")\n",
    "print(x[:5])\n",
    "print(\"Pr[H_i|x_i,x_-i], j-th row is for j-th sample and i=positions[j], only showing first 5:\")\n",
    "print(hh[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linearnetwork = nn.Sequential(\n",
    "            nn.Linear(num_obs, num_hidden_state, bias=False)\n",
    "        )\n",
    "        self.linearnetwork_h = nn.Sequential(\n",
    "            nn.Linear(num_obs, num_hidden_state, bias=False)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, ind_x):\n",
    "        logits = self.linearnetwork(ind_x)\n",
    "        h = self.linearnetwork_h(x)\n",
    "        return nn.Softmax(dim = 1)(torch.log(h) + logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_one_hot = np.zeros((num_samples, num_obs))\n",
    "for i in range(num_samples):\n",
    "    x_one_hot[i, int(obs[i, pos[i]])] = 1\n",
    "#print(pos[:9])\n",
    "#print(obs[:9])\n",
    "#print(x_one_hot[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "features1, features2, labels = x, x_one_hot, hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters.\n",
    "lr = 1\n",
    "epochs = 10000\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.TensorDataset(torch.FloatTensor(features1), torch.FloatTensor(features2), torch.FloatTensor(labels))\n",
    "train_dl = data.DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = NeuralNetwork()\n",
    "nn.init.constant_(net.linearnetwork_h[0].weight, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.KLDivLoss(reduction='batchmean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = torch.optim.SGD(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "loss:  tensor(2.1198, grad_fn=<AddBackward0>)\n",
      "epoch:  100\n",
      "loss:  tensor(0.0580, grad_fn=<AddBackward0>)\n",
      "epoch:  200\n",
      "loss:  tensor(0.0421, grad_fn=<AddBackward0>)\n",
      "epoch:  300\n",
      "loss:  tensor(0.0329, grad_fn=<AddBackward0>)\n",
      "epoch:  400\n",
      "loss:  tensor(0.0260, grad_fn=<AddBackward0>)\n",
      "epoch:  500\n",
      "loss:  tensor(0.0209, grad_fn=<AddBackward0>)\n",
      "epoch:  600\n",
      "loss:  tensor(0.0169, grad_fn=<AddBackward0>)\n",
      "epoch:  700\n",
      "loss:  tensor(0.0140, grad_fn=<AddBackward0>)\n",
      "epoch:  800\n",
      "loss:  tensor(0.0117, grad_fn=<AddBackward0>)\n",
      "epoch:  900\n",
      "loss:  tensor(0.0099, grad_fn=<AddBackward0>)\n",
      "epoch:  1000\n",
      "loss:  tensor(0.0085, grad_fn=<AddBackward0>)\n",
      "epoch:  1100\n",
      "loss:  tensor(0.0073, grad_fn=<AddBackward0>)\n",
      "epoch:  1200\n",
      "loss:  tensor(0.0064, grad_fn=<AddBackward0>)\n",
      "epoch:  1300\n",
      "loss:  tensor(0.0056, grad_fn=<AddBackward0>)\n",
      "epoch:  1400\n",
      "loss:  tensor(0.0049, grad_fn=<AddBackward0>)\n",
      "epoch:  1500\n",
      "loss:  tensor(0.0042, grad_fn=<AddBackward0>)\n",
      "epoch:  1600\n",
      "loss:  tensor(0.0037, grad_fn=<AddBackward0>)\n",
      "epoch:  1700\n",
      "loss:  tensor(0.0032, grad_fn=<AddBackward0>)\n",
      "epoch:  1800\n",
      "loss:  tensor(0.0028, grad_fn=<AddBackward0>)\n",
      "epoch:  1900\n",
      "loss:  tensor(0.0024, grad_fn=<AddBackward0>)\n",
      "epoch:  2000\n",
      "loss:  tensor(0.0021, grad_fn=<AddBackward0>)\n",
      "epoch:  2100\n",
      "loss:  tensor(0.0018, grad_fn=<AddBackward0>)\n",
      "epoch:  2200\n",
      "loss:  tensor(0.0015, grad_fn=<AddBackward0>)\n",
      "epoch:  2300\n",
      "loss:  tensor(0.0013, grad_fn=<AddBackward0>)\n",
      "epoch:  2400\n",
      "loss:  tensor(0.0012, grad_fn=<AddBackward0>)\n",
      "epoch:  2500\n",
      "loss:  tensor(0.0011, grad_fn=<AddBackward0>)\n",
      "epoch:  2600\n",
      "loss:  tensor(0.0010, grad_fn=<AddBackward0>)\n",
      "epoch:  2700\n",
      "loss:  tensor(0.0009, grad_fn=<AddBackward0>)\n",
      "epoch:  2800\n",
      "loss:  tensor(0.0008, grad_fn=<AddBackward0>)\n",
      "epoch:  2900\n",
      "loss:  tensor(0.0008, grad_fn=<AddBackward0>)\n",
      "epoch:  3000\n",
      "loss:  tensor(0.0007, grad_fn=<AddBackward0>)\n",
      "epoch:  3100\n",
      "loss:  tensor(0.0007, grad_fn=<AddBackward0>)\n",
      "epoch:  3200\n",
      "loss:  tensor(0.0007, grad_fn=<AddBackward0>)\n",
      "epoch:  3300\n",
      "loss:  tensor(0.0006, grad_fn=<AddBackward0>)\n",
      "epoch:  3400\n",
      "loss:  tensor(0.0006, grad_fn=<AddBackward0>)\n",
      "epoch:  3500\n",
      "loss:  tensor(0.0006, grad_fn=<AddBackward0>)\n",
      "epoch:  3600\n",
      "loss:  tensor(0.0006, grad_fn=<AddBackward0>)\n",
      "epoch:  3700\n",
      "loss:  tensor(0.0005, grad_fn=<AddBackward0>)\n",
      "epoch:  3800\n",
      "loss:  tensor(0.0005, grad_fn=<AddBackward0>)\n",
      "epoch:  3900\n",
      "loss:  tensor(0.0005, grad_fn=<AddBackward0>)\n",
      "epoch:  4000\n",
      "loss:  tensor(0.0005, grad_fn=<AddBackward0>)\n",
      "epoch:  4100\n",
      "loss:  tensor(0.0005, grad_fn=<AddBackward0>)\n",
      "epoch:  4200\n",
      "loss:  tensor(0.0005, grad_fn=<AddBackward0>)\n",
      "epoch:  4300\n",
      "loss:  tensor(0.0005, grad_fn=<AddBackward0>)\n",
      "epoch:  4400\n",
      "loss:  tensor(0.0005, grad_fn=<AddBackward0>)\n",
      "epoch:  4500\n",
      "loss:  tensor(0.0004, grad_fn=<AddBackward0>)\n",
      "epoch:  4600\n",
      "loss:  tensor(0.0004, grad_fn=<AddBackward0>)\n",
      "epoch:  4700\n",
      "loss:  tensor(0.0004, grad_fn=<AddBackward0>)\n",
      "epoch:  4800\n",
      "loss:  tensor(0.0004, grad_fn=<AddBackward0>)\n",
      "epoch:  4900\n",
      "loss:  tensor(0.0004, grad_fn=<AddBackward0>)\n",
      "epoch:  5000\n",
      "loss:  tensor(0.0004, grad_fn=<AddBackward0>)\n",
      "epoch:  5100\n",
      "loss:  tensor(0.0004, grad_fn=<AddBackward0>)\n",
      "epoch:  5200\n",
      "loss:  tensor(0.0004, grad_fn=<AddBackward0>)\n",
      "epoch:  5300\n",
      "loss:  tensor(0.0004, grad_fn=<AddBackward0>)\n",
      "epoch:  5400\n",
      "loss:  tensor(0.0004, grad_fn=<AddBackward0>)\n",
      "epoch:  5500\n",
      "loss:  tensor(0.0004, grad_fn=<AddBackward0>)\n",
      "epoch:  5600\n",
      "loss:  tensor(0.0004, grad_fn=<AddBackward0>)\n",
      "epoch:  5700\n",
      "loss:  tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch:  5800\n",
      "loss:  tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch:  5900\n",
      "loss:  tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch:  6000\n",
      "loss:  tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch:  6100\n",
      "loss:  tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch:  6200\n",
      "loss:  tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch:  6300\n",
      "loss:  tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch:  6400\n",
      "loss:  tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch:  6500\n",
      "loss:  tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch:  6600\n",
      "loss:  tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch:  6700\n",
      "loss:  tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch:  6800\n",
      "loss:  tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch:  6900\n",
      "loss:  tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch:  7000\n",
      "loss:  tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch:  7100\n",
      "loss:  tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch:  7200\n",
      "loss:  tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch:  7300\n",
      "loss:  tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch:  7400\n",
      "loss:  tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch:  7500\n",
      "loss:  tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch:  7600\n",
      "loss:  tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch:  7700\n",
      "loss:  tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "epoch:  7800\n",
      "loss:  tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch:  7900\n",
      "loss:  tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch:  8000\n",
      "loss:  tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch:  8100\n",
      "loss:  tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch:  8200\n",
      "loss:  tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch:  8300\n",
      "loss:  tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch:  8400\n",
      "loss:  tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch:  8500\n",
      "loss:  tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch:  8600\n",
      "loss:  tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch:  8700\n",
      "loss:  tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch:  8800\n",
      "loss:  tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch:  8900\n",
      "loss:  tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch:  9000\n",
      "loss:  tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch:  9100\n",
      "loss:  tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch:  9200\n",
      "loss:  tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch:  9300\n",
      "loss:  tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch:  9400\n",
      "loss:  tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch:  9500\n",
      "loss:  tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch:  9600\n",
      "loss:  tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch:  9700\n",
      "loss:  tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch:  9800\n",
      "loss:  tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "epoch:  9900\n",
      "loss:  tensor(0.0002, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "total_loss_lst = []\n",
    "for i in range(epochs):\n",
    "    total_loss = 0\n",
    "    for X1, X2, y in train_dl:\n",
    "        l = loss(torch.log(net(X1, X2)), y)\n",
    "        #l = shiftedNegLLLoss(net(X1, X2), y)\n",
    "        total_loss += l\n",
    "        trainer.zero_grad()\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "    if (i % 100 == 0):\n",
    "        print(\"epoch: \", i)\n",
    "        print(\"loss: \", total_loss)\n",
    "        total_loss_lst.append(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEGCAYAAABy53LJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3dd3xc1Zn/8c+jURl1S7Zky5YrLmAwuCgBEggESAIkhoQQAgHSKCHZsEl2f7uBTSNtSdlld5OlBtgQigkhbMCEUEIoYWm2IeCGccNYbnJVtSRLen5/zJURskaasWY8o9H3/XrNa2au7tx5jmz85Zxz7z3m7oiIiMQqK9UFiIjI0KLgEBGRuCg4REQkLgoOERGJi4JDRETikp3qApJh1KhRPmnSpFSXISIypCxZsmSHu1cMtF9GBsekSZNYvHhxqssQERlSzGxDLPtpqEpEROKSUcFhZvPN7Jb6+vpUlyIikrEyKjjcfaG7X15aWprqUkREMlZGBYeIiCSfgkNEROKi4BARkbgoOEREJC4Kjh5+v6SWe156O9VliIikNQVHDwtf38y9ixQcIiL9yajgGOx1HAW5IVraOxNclYhIZsmo4BjsdRzhnBB7FRwiIv3KqOAYrPycEK37FBwiIv1RcPRQkBtir4JDRKRfCo4e8nMiweHuqS5FRCRtKTh6COeGcIe2jq5UlyIikrYUHD0U5IQANEEuItIPBUcP+bmR4GjRPIeISFQKjh7C6nGIiAxIwdFDfhAcOiVXRCQ6BUcPBbmRJdh19biISHQKjh7ycyO/Dl3LISISXXaqCxiImRUCNwDtwNPufneyvktzHCIiA0tJj8PMbjezOjNb1mv76Wa2yszWmNlVweZzgPvd/TLgrGTW1T1UpTkOEZHoUjVU9Wvg9J4bzCwEXA+cAcwELjCzmUA1sDHYLan/ondPjmuOQ0QkupQEh7s/C+zqtfm9wBp3X+fu7cC9wNlALZHwgH7qNbPLzWyxmS3evn37QdXVHRya4xARiS6dJsfH8U7PAiKBMQ54APikmd0ILIz2YXe/xd1r3L2moqLioArovgBQQ1UiItGl0+S49bHN3b0Z+EJMBzCbD8yfOnXqQRWQEzJCWUZLe8dBfV5EZDhIpx5HLTC+x/tqYHM8BxjsQk5mFrlDbrtucigiEk06BcciYJqZTTazXOB84KFDXUQ4R2tyiIj0J1Wn4y4AXgBmmFmtmV3i7h3AV4HHgJXAfe6+PM7jDmrNcQgWc9JQlYhIVCmZ43D3C6JsfwR4ZBDHXQgsrKmpuexgj5GvHoeISL/SaagqLYRzQ+zdpzkOEZFoMio4EjJUlROiVRcAiohElVHBMdizqiByLUfLPs1xiIhEk1HBkQiR03HV4xARiSajgiMRQ1X5uSFaNcchIhJVRgVHQoaqckK6clxEpB8ZFRyJkJ+r03FFRPqj4OglnBMZqurq8lSXIiKSljIqOBJ15ThAa4d6HSIifcmo4EjUHAdo+VgRkWgyKjgSQYs5iYj0T8HRixZzEhHpn4KjF607LiLSv4wKjkRdAAia4xARiSajgiNR96oCzXGIiESTUcGRCDqrSkSkfwqOXnRWlYhI/xQcvRRoqEpEpF8Kjl7CmhwXEelXRgVHQs6q0hyHiEi/Mio4EnFWVU4oi+ws01CViEgUGRUciaJbq4uIRKfg6IOWjxURiU7B0Qf1OEREolNw9EE9DhGR6BQcfVCPQ0QkOgVHH9TjEBGJLqOCIxHXcUDk6nH1OERE+pZRwZGI6zgAwjkKDhGRaDIqOBJFQ1UiItEpOPqgyXERkegUHH3Iz1WPQ0QkGgVHH/JzQrR1dNHZ5akuRUQk7Sg4+tB9h9xWDVeJiBxAwdEHLeYkIhKdgqMPYa3JISISlYKjD/nqcYiIRKXg6EOBlo8VEYlKwdGH/UNV6nGIiBwg7YPDzKaY2W1mdv+h+k6tOy4iEl1Sg8PMbjezOjNb1mv76Wa2yszWmNlV/R3D3de5+yXJrLM3zXGIiESXneTj/xr4b+A33RvMLARcD3wIqAUWmdlDQAi4ttfnv+judUmu8QAFOZFfi3ocIiIHSmpwuPuzZjap1+b3AmvcfR2Amd0LnO3u1wIfO9jvMrPLgcsBJkyYcLCHASCcG+mItajHISJygFTMcYwDNvZ4Xxts65OZjTSzm4A5ZnZ1tP3c/RZ3r3H3moqKikEVuP/KcfU4REQOkOyhqr5YH9ui3hTK3XcCV8R0YLP5wPypU6ceZGkR+TqrSkQkqlT0OGqB8T3eVwObE3HgRC3klB3KIjeUpeAQEelDKoJjETDNzCabWS5wPvBQCuroVzgnS5PjIiJ9SPbpuAuAF4AZZlZrZpe4ewfwVeAxYCVwn7svT9D3JWTNcYCC3GwFh4hIHwYMDjP7mZmVmFmOmT1pZjvM7KJYDu7uF7h7lbvnuHu1u98WbH/E3ae7+2Hu/uPBNqLH9yVkqAq0CqCISDSx9Dg+7O4NRE6VrQWmA/+U1KrSQDgnRIt6HCIiB4glOHKC5zOBBe6+K4n1DEpih6pCWshJRKQPsQTHQjN7A6gBnjSzCqA1uWUdnIQOVeVoqEpEpC8DBoe7XwUcD9S4+z6gGTg72YWlmoaqRET6Fsvk+KeADnfvNLNvA3cBY5Ne2UFI5FBVUV6IHU1tdHZFvTZRRGRYimWo6jvu3mhmJwAfAe4AbkxuWQcnkUNVp80czfbGNv68clsCKhMRyRyxBEf3eM1HgRvd/UEgN3klpYfTjxxDdVk+v3p2XapLERFJK7EExyYzuxk4D3jEzPJi/NyQlh3K4pITJrN4w25eeXt3qssREUkbsQTAeUSu8j7d3fcA5aTpdRyJnOMAOK9mPCXhbG79q3odIiLdYjmrqgVYC3zEzL4KVLr740mv7CAkco4DoDAvmwuPm8ijy7ayYWdzQo4pIjLUxXJW1deAu4HK4HGXmV2Z7MLSxeffN4lQlnH7c+tTXYqISFqIZajqEuBYd/+uu38XOA64LLllpY/RJWE+Pnsc9y7ayKY9e1NdjohIysUSHMY7Z1YRvO5rMaaM9fUPTccM/vWRlakuRUQk5WIJjv8BXjKza8zsGuBF4LakVnWQEj053m3ciHyuOOkw/vj6Fl5ctzOhxxYRGWrMfeAro81sLnACkZ7Gs+7+arILG4yamhpfvHhxQo+5t72T0657huJwNg9feQLZoYw/I1lEhhkzW+LuNQPtF/VfPzMr734AbxG51cidwIZg27CSnxviX848gje2NrJg0cZUlyMikjLZ/fxsCeC8M5/R3TWx4PWUJNaVls6cNYZjJ5fz74+v4qOzqigvzPgL6EVEDhC1x+Huk919SvDc/br7/bALDQAz4wdnH0VTawfXaqJcRIYpDdTHacaYYi49cQq/W1KriXIRGZYyKjiSdVZVb187dRrVZfl863+X0tahNTtEZHjJqOBI9C1HosnPDfHDs49i7fZmbnlG97ESkeEllluOlPfxyBnoc5nug4dXcuasMfzyqTWs296U6nJERA6ZWHocrwDbgTeB1cHr9Wb2ipnNS2Zx6e6a+UcSzs7im79/nS6tFCgiw0QswfEocKa7j3L3kcAZwH3AV4AbkllcuqssCfOdj81k0Vu7ueulDakuR0TkkIglOGrc/bHuN8Et1T/g7i8CeUmrbIg4d141H5hewU//9Aa1u1tSXY6ISNLFEhy7zOybZjYxePwzsNvMQkBXkutLe2bGv37iKACufmApsdzCRURkKIslOD4DVAN/AB4EJgTbQkRWBxz2qssKuOqMw/nr6h0seFm3IxGRzNbfLUcAcPcdQLSFm9YktpzBMbP5wPypU6ce8u++8NiJPLZ8Gz98eAXvO2wkk0YVHvIaREQOhVhOx51uZreY2eNm9pfux6EoLl6H6jqOvmRlGT//1NHkhIxv3Pc3OjqH/SieiGSoWIaqfge8Cnwb+KceD+mlqjSfH31iFq++vYcbn16b6nJERJJiwKEqoMPdb0x6JRnirGPG8sSKbfzXk6s5cXoFs8ePSHVJIiIJFUuPY6GZfcXMqnqt0SFR/OjsoxhdEuar97xCfcu+VJcjIpJQsQTH54gMTT1PZI2OJUBil9fLMKUFOfzyM3PYWt/KP93/mk7RFZGMMmBw9FiPo+djWK7HEY+5E8q46ozDeXzFNn79/FupLkdEJGGiznGY2Snu/hczO6evn7v7A8krKzNccsJkXly3k399ZCWzx49gzoSyVJckIjJo/fU4Tgqe5/fx+FiS68oIZsa/feoYxpSG+dKdS9jW0JrqkkREBs0ycfy9pqbGFy9On2mYlVsaOOeG5zm8qph7Lz+OvOxQqksSETmAmS1x95qB9ovlAsA8M/uMmf2LmX23+5GYMoeHI6pKuO68Y3j17T185w/LNFkuIkNaLGdVPQicDXQAzT0eEoczZlVx5SlTuW9xLbc9tz7V5YiIHLRYLgCsdvfTk15JFGb2ceCjQCVwfXBb9yHpG6dNZ01dEz9+ZCVjR+Rz5qyqVJckIhK3WHocz5vZrIM5uJndbmZ1Zras1/bTzWyVma0xs6v6O4a7/8HdLwM+D3z6YOpIF1lZxn98ejZzJ5Tx9d/+jcVv7Up1SSIicYslOE4AlgT/0L9uZkvN7PUYj/9r4F29lWAdj+uJrCQ4E7jAzGaa2Swze7jXo7LHR78dfG5IC+eE+NVnaxg3Ip9Lf7OYtVqvXESGmAHPqjKziX1td/eY1ko1s0nAw+5+VPD+eOAad/9I8P7q4HjXRvm8AT8BnnD3P/fzPZcDlwNMmDBh3oYN6b2U69s7Wzjnxv8jN5TF7778PsaNyE91SSIyzA36rCozKwleNkZ5HKxxQM/VjmqDbdFcCZwGnGtmV0Tbyd1vcfcad6+pqKgYRHmHxoSRBdzxxffS2NbBRbe+xPbGtlSXJCISk/6Gqu4JnrvvTbWExNyryvrYFrXb4+6/cPd57n6Fu9/U74HN5pvZLfX19YMo79A5cmwpv/7Ce9ha38rFt72kGyKKyJAQNTjc/WPB82R3n5LAe1XVAuN7vK8GNg/iePulciGngzVvYjm3fHYe67Y389nbX6J+r8JDRNJbLJPjmFmZmb3XzD7Q/RjEdy4CppnZZDPLBc4HHhrE8Ya8E6dVcMOFc1mxpYHP3v4yDa0KDxFJX7FcOX4p8CzwGPD94PmaWA5uZguAF4AZZlZrZpe4ewfw1eA4K4H73H35wZV/wPcNqaGqnk6bOZobLpzHis31XHybwkNE0lcsZ1UtBd4DvOjus83scOD77p6211Sk272q4vH48q383T2vMLOqhN988VhKC3JSXZKIDBMJu1cV0OrurcFB89z9DWDGYAuUvn34yDHceOE8Vm5p5IJfvcjOJp1tJSLpJZbgqDWzEcAfgCfM7EESNJmdaEN5qKqn02aO5lefq2Ht9ibOv+VF6hp1O3YRSR9x3VbdzE4CSoFH3b09aVUN0lAequrphbU7ueSORVQW53HXpcdSXVaQ6pJEJIMlZKjKzLJ63mfK3Z9x94fSOTQyyfGHjeTOS45lV3M75930gm5PIiJpod/gcPcu4DUzm3CI6hmUTBmq6mnexDLuvfx42ju7OO+mF1i+OXPaJiJDUyxzHFXAcjN70swe6n4ku7CDMRQvAIzFzLEl3Pel48nLzuL8W17UXXVFJKViOR33pL62u/szSakoATJljqO3TXv2cvGtL7G5fi83XTSPk2dUDvwhEZEYJfJ03DODuY39D+DMwZco8Ro3Ip/7rjieKaOKuOw3i/nj61tSXZKIDEOxBMeH+th2RqILSYRMnOPobVRRHgsuP45jqkdw5YJXuG/RxoE/JCKSQP3dVv3LwVXjM4IFnLof64FYF3I6pDJ1jqO30vwc7rzkWE6YVsE///51bv3rulSXJCLDSH9rjt8D/Am4Fui5vGuju2t2NsXyc0P86rPz+Pq9f+NHf1xJQ2sH3zhtGpF1r0REkidqcLh7PVAPXHDoypF45GWH+OUFc7jqgaX84snV7G3v4F/OPELhISJJ1V+PY8gxs/nA/KlTp6a6lEMmO5TFzz55NIW5IX711/W0tHfyw7OPIitL4SEiyRHTehxDxXCZ4+gtK8u45qwj+dJJU7j7pbf5f/e/RmdX7LeSERGJR0b1OIYzM+Oq0w+nMDeb6554k45O57rzjiE7lFH/byAiaUDBkUHMjL8/dRo5oSx++ugbdHY5/3n+bHIUHiKSQAqODPTlkw8jJ2T86I8r6ejq4pcXzCU3W+EhIomhf00y1KUnTuF782fy2PJtXLngFfZ1dqW6JBHJEAqODPaF909+JzzueVXhISIJkVHBMRxuORKvL7x/Mt/92EweXb6Vv1+g8BCRwcuo4Biup+MO5IsnTObbHz2CPy3byj/cp1N1RWRwNDk+TFx64hQ6upyf/OkNckNZ/Pzco3WRoIgcFAXHMHLFSYfR3tHFdU+8SW628eOPz1J4iEjcFBzDzJWnTKWto5Prn1pLXnaI782fqXtbiUhcFBzDjJnx/z48g7Z9Xdz63HrycrK46vTDFR4iEjMFxzBkZnzro0ewd18nNz+zjoKcbL522rRUlyUiQ4SCY5gyM3549lG0dXTxH39+k7ycLK446bBUlyUiQ0BGBcdwvK36YGRlGT/95NG0dXTtP9vqiydMTnVZIpLmdB3HMBfKMq477xhOP3IMP3h4BXe9uCHVJYlImsuo4JCDkxPK4hcXzOHUwyv59h+W8dtFb6e6JBFJYwoOASA3O4vrL5zLSdMr+Obvl3LvywoPEembgkP2C+eEuPnieZw8o4KrHljKAoWHiPRBwSHvEs4JcdNF8/jgjAqufmApd77wVqpLEpE0o+CQA4RzQtx08TxOO2I033lwOdc/tQZ33RhRRCIUHNKnvOwQN140l0/MGcfPH1vFtX96Q+EhIkCGXcchiZUTyuLfP3UMxeFsbnl2HTua2vjJOUdrGVqRYU7BIf3KyjK+f9aRjCrK47on3mTT7r3cfPE8RhTkpro0EUkR/a+jDMjM+PtTp/Ff58/m1bf3cM4Nz7N+R3OqyxKRFFFwSMzOnj2Ouy87lt0t7Zz1y+d4dNmWVJckIimQ9sFhZkeY2U1mdr+ZfTnV9Qx375lUzsIrT2BKZRFX3PUKP1i4gvYOrWMuMpwkNTjM7HYzqzOzZb22n25mq8xsjZld1d8x3H2lu18BnAfUJLNeiU11WQG/+9LxfP59k7j9/9bzyRufZ9XWxlSXJSKHSLJ7HL8GTu+5wcxCwPXAGcBM4AIzm2lms8zs4V6PyuAzZwHPAU8muV6JUW52FtecdSQ3XjiXTXv2Mv+Xz3H9U2vo6FTvQyTTWbLPzTezScDD7n5U8P544Bp3/0jw/moAd782hmP90d0/GuVnlwOXA0yYMGHehg26y+uhsqOpje8+uIxHlm5lZlUJ3z/7SN4zqTzVZYlInMxsibsPOLKTijmOccDGHu9rg219MrOTzewXZnYz8Ei0/dz9FnevcfeaioqKxFUrAxpVlMcNF87jhgvnsrulnU/d9AJXLniVzXv2pro0EUmCVFzH0dfi1lG7Pe7+NPB0TAfWQk4pdeasKk6eUcFNT6/l5mfX8diyrXzm2Al85eTDqCwJp7o8EUmQVPQ4aoHxPd5XA5sTcWAt5JR6BbnZ/MOHZ/DkP57EJ+aM484XN3Diz57imoeWs3FXS6rLE5EESEVwLAKmmdlkM8sFzgceSkEdkkTVZQX89Nyj+cs/nsT8Y8Zy14sbOOnnT/GVu5ewZMMu3fdKZAhL6uS4mS0ATgZGAduA77n7bWZ2JvCfQAi43d1/nKDv6x6qumz16tWJOKQkyJb6vdzx/AbueWkDDa0dTB9dxPnvmcA5c8fp9iUiaSLWyfGkn1WVCjU1Nb548eJUlyF9aG7rYOFrm1nw8tu8VltPbiiLk2dU8PE54zjl8ErCOaFUlygybA3L4FCPY2hZsbmB+5fUsvD1zWxvbKMoL5tTDq/kjKPGcNKMCgpydQ9OkUNpWAZHN/U4hpbOLueFtTtZ+NpmHl+xld0t+wjnZPH+w0ZxyhGVnHJ4JVWl+akuUyTjKTgUHENSR2cXL6/fxWPLt/KXVXVs3BW5FmT66CJOnFbBidNG8d7J5eqNiCSBgkPBMeS5O2u3N/GXN+r46+odvLR+F+0dXeSEjNnjR3D8lJEcO2UkcyaMUJCIJMCwDA7NcWS21n2dvLx+F8+v3ckLa3ewdFM9XQ7ZWcas6lJqJpYxb2I58yaWUVGcl+pyRYacYRkc3dTjGB4aWvexZMNuXl6/i0Xrd/F6bT3twU0Wq8vymTOhjNnjRzB7fCkzq0rJz9UZWyL9iTU41L+XIasknMMHZ1TywRmVALR1dLJsUwNLNuzibxv3sOStXSx8LXJTglCWMa2yiKOrS5k1rpQjx5VyxJgShYnIQVBwSMbIyw4xb2IZ8yaW7d+2raGV12vreb12D6/V1vPnlXXct7gWgCyDyaMKOXJsKUdUlXBEVTFHVJVQWZyHWV+3VBMRyLChKs1xyEDcnS31rSzdVM+KzQ0s39zAis31bK5v3b9PeWEu00cXcfiYEqaPLmb66CKmjS6mND8nhZWLJJ/mODTHIXGob9nHyq0NvLGlgTe2NrJqWyNvbm2kub1z/z6jS/KYVlnM1Mqi/Y/DKooYVZSrHopkBM1xiMShtCCH46aM5LgpI/dv6+pyNu3Zy+q6Rt7c1sSb2xpZW9fEfYs30tIjUErC2RxWWcSUUUVMqSjksIpCJo8qYuLIAt1CRTKSgkMkiqwsY3x5AePLCzjl8NH7t3d1OVsaWllb18SauibWbm9i3fZm/rp6O79/pXb/fmYwtjSfSaMKmDSykMmjCpk0spBJoyLHzMtWqMjQlFHBoYWc5FDIyjLGjchn3Ih8PjD93atNNrbu460dLazb0cT6Hc28taOZ9TtbePj1LdTv3bd/v+5QmVBewMSRBUwYWcDE8kImlBcwobyA0gLNp0j60hyHyCGyp6U9EiY7m9mws4UNO1t4a2czG3e1sKOp/V37loSzmTCygPFlkSCpLi9gfFk+1WUFVJflawhMkkJzHCJpZkRBLnMm5DJnQtkBP2tq6+DtnS1s3N3Cxl2RUHl7VwurtjXy5Bt1tHd0vWv/yuI8qoMgGVeWT3VZpAdUXVbAuBH5uj5FkkrBIZIGivKymTm2hJljSw74WVeXs72pjY27WqjdvZeNuyIBU7t7L69u3M0jS7fQ0fXukYORhbmMK8tnbGl+5HlEd7BEXpcV5OhMMDloCg6RNJeVZYwuCTO6JEzNpAN/3tnl1DW2Urt7L5t272XTnr3U7m5h055WVtc18vSbdbTue3ePJZyTxdgRkWCpKg1TNSKfsb2ei/L0z4P0TX8zRIa4UJZRVZpPVWk+75l04M/dnd0t+4JQaWHznlY279nL5vq9bN7TyrOrt1PX2Ebv6c7icHYkVErzGTsizJiSfKpGhINtYcaUKlyGq4z6U9dZVSIHMjPKC3MpL8xlVnVpn/vs6+xiW0Mrm/e0siUIlK31e9lc38rW+laWb25gR1PbAZ8rystmTGmYMUGPqKo0zOjgfVVpZNvIwlyysjQslkl0VpWIxKSto5O6hja2NkR6LNsaWtkSBMvWhshzXWMbnb3mW7KzjMriPEaXhhldHGZMaZjKkjxGF4eDIbg8KkvClISzNe+SYjqrSkQSKi87tP+CyGg6u5wdTW1srY+EyraG7kcb2xpaWbO9if9bu4PG1o4DPhvOyYoESXGYipK8SNiUhKkszqOyOAiY4jAl+QqYVFNwiEjChHpM5B8zPvp+Le0d1AVhsrWhle2N3a/bqGtoZeXmBp5uaH3XvcK65WVnUVGcF3kU5VFZkkdFUaQXU9m9vTiPkYV55GZnJbG1w5eCQ0QOuYLcbCaNymbSqMJ+92tq66Au6LHUNUYCpq6xLXhuZf2OZha9tYvdLfv6/HxZQQ6VxeH9YTKqKDd4ztv/PKooj/LCXEKah4mZgkNE0lZRXjZFFUVMqSjqd7+2jk52NLWzvUeovPM68vzWzma2N7bR1utiSoiszVJemNsrUCLvRxblMbIol4rgWT0ZBYeIZIC87ND++4f1x91pauvYHzI7miKP7tfbG9vZ0dTGuu3N7GxuO+D6l24l4WxGFecxqjAIkyBQRhXlMjLowYwsjLwekZ+TcWeVKThEZNgwM4rDORSHc5g8wDCZu9Pc3smOxjZ2Nrezs6mNHU3dz8Hr5jbW1DXx4ro29uzdd8C1MNDdm8ljZHBKdHlR7v7XIwtzKdv/OhI4ZQU5ZIfSu0ej4BAR6YOZRYbK8gaeiwHo6Oxid8s+dja3saupnR1B2OxqbmdHU6Qns6u5nRWbG9jZ1EZDH2eWdSvNz9kfIuWFeZQXvvNcVhAJmrLCXMoLIs+H+lTmjAoOXQAoIqmSHXrnbK9Y7OvsYk/LPnY1R3ouu5rb9z92N7ezM3hdu7uFpZsir/d19n3dXXaWMaIgEjS3fq6GiSMHDrrByKjgcPeFwMKamprLUl2LiEh/ct4VNMUD7t89P7O7OdKr2dOyj90tQdC0tLOreR+7m9spPAS3gcmo4BARyVQ952cmjIx+EeahkN4zMCIiknYUHCIiEhcFh4iIxEXBISIicVFwiIhIXBQcIiISFwWHiIjERcEhIiJxycilY81sO7DhID8+CtiRwHKGiuHYbrV5+BiO7T6YNk9094qBdsrI4BgMM1scy5q7mWY4tlttHj6GY7uT2WYNVYmISFwUHCIiEhcFx4FuSXUBKTIc2602Dx/Dsd1Ja7PmOEREJC7qcYiISFwUHCIiEhcFR8DMTjezVWa2xsyuSnU9g2Vm483sKTNbaWbLzexrwfZyM3vCzFYHz2U9PnN10P5VZvaRHtvnmdnS4Ge/sEO5uPFBMLOQmb1qZg8H7zO6zWY2wszuN7M3gj/v4zO9zQBm9o3g7/YyM1tgZuFMbLeZ3W5mdWa2rMe2hLXTzPLM7LfB9pfMbNKARbn7sH8AIWAtMAXIBV4DZqa6rkG2qQqYG7wuBt4EZgI/A64Ktl8F/DR4PTNodx4wOfh9hIKfvQwcDxjwJ+CMVLdvgLb/A3AP8HDwPqPbDNwBXBq8zgVGDCZ62EQAAATXSURBVIM2jwPWA/nB+/uAz2diu4EPAHOBZT22JaydwFeAm4LX5wO/HbCmVP9S0uER/DIf6/H+auDqVNeV4DY+CHwIWAVUBduqgFV9tRl4LPi9VAFv9Nh+AXBzqtvTTzurgSeBU3oER8a2GSgJ/gG1Xtszts1BfeOAjUA5kSWwHwY+nKntBib1Co6EtbN7n+B1NpGrza2/ejRUFdH9l7BbbbAtIwRdzznAS8Bod98CEDxXBrtF+x2MC1733p6u/hP4Z6Crx7ZMbvMUYDvwP8Hw3K1mVkhmtxl33wT8G/A2sAWod/fHyfB295DIdu7/jLt3APXAyP6+XMER0deYZkacp2xmRcDvga+7e0N/u/axzfvZnnbM7GNAnbsvifUjfWwbUm0m8n+Ic4Eb3X0O0Exk6CKaTGgzwZj+2USGY8YChWZ2UX8f6WPbkGt3DA6mnXH/DhQcEbXA+B7vq4HNKaolYcwsh0ho3O3uDwSbt5lZVfDzKqAu2B7td1AbvO69PR29HzjLzN4C7gVOMbO7yOw21wK17v5S8P5+IkGSyW0GOA1Y7+7b3X0f8ADwPjK/3d0S2c79nzGzbKAU2NXflys4IhYB08xsspnlEpkgeijFNQ1KcMbEbcBKd7+ux48eAj4XvP4ckbmP7u3nB2dYTAamAS8H3eBGMzsuOOZne3wmrbj71e5e7e6TiPwZ/sXdLyKz27wV2GhmM4JNpwIryOA2B94GjjOzgqDeU4GVZH67uyWynT2PdS6R/27673WletInXR7AmUTOPFoLfCvV9SSgPScQ6W6+DvwteJxJZOzySWB18Fze4zPfCtq/ih5nlgA1wLLgZ//NABNn6fAATuadyfGMbjMwG1gc/Fn/ASjL9DYH9X4feCOo+U4iZxJlXLuBBUTmcfYR6R1cksh2AmHgd8AaImdeTRmoJt1yRERE4qKhKhERiYuCQ0RE4qLgEBGRuCg4REQkLgoOERGJi4JDJA2Y2ckW3M1XJN0pOEREJC4KDpE4mNlFZvaymf3NzG62yNofTWb272b2ipk9aWYVwb6zzexFM3vdzP63e80EM5tqZn82s9eCzxwWHL7I3llX4+4e6yX8xMxWBMf5txQ1XWQ/BYdIjMzsCODTwPvdfTbQCVwIFAKvuPtc4Bnge8FHfgN8092PBpb22H43cL27H0Pk/kpbgu1zgK8TWVNhCvB+MysHPgEcGRznR8ltpcjAFBwisTsVmAcsMrO/Be+nELmF+2+Dfe4CTjCzUmCEuz8TbL8D+ICZFQPj3P1/Ady91d1bgn1edvdad+8icouYSUAD0ArcambnAN37iqSMgkMkdgbc4e6zg8cMd7+mj/36u49Pf8uStvV43Qlke2R9hPcSucvxx4FH46xZJOEUHCKxexI418wqYf+6zxOJ/Hd0brDPZ4Dn3L0e2G1mJwbbLwae8ciaKLVm9vHgGHlmVhDtC4P1VErd/REiw1izk9EwkXhkp7oAkaHC3VeY2beBx80si8jdSv+OyOJJR5rZEiKrp306+MjngJuCYFgHfCHYfjFws5n9IDjGp/r52mLgQTMLE+mtfCPBzRKJm+6OKzJIZtbk7kWprkPkUNFQlYiIxEU9DhERiYt6HCIiEhcFh4iIxEXBISIicVFwiIhIXBQcIiISl/8PS8hf6nOWWJIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x_lst = list(range(0, 10000, 100))\n",
    "plt.plot(x_lst, total_loss_lst)\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"training loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned matrix:\n",
      "[[ 0.89628845 -1.1770345  -0.9938493   3.8384502   0.6095001 ]\n",
      " [ 0.8236614   0.34915718 -0.21126103  1.3891723   3.6333191 ]\n",
      " [-2.3793757   0.32312828  0.4282125  -5.0138817  -4.623234  ]]\n",
      "log(O):\n",
      "[[ -1.25729566  -1.38032328  -4.32893177  -0.82547526  -4.35268422]\n",
      " [ -1.89086747  -0.41556183  -4.10711787  -3.83531198  -1.89021013]\n",
      " [ -4.70896735  -0.05633851  -3.08419628 -23.26446841 -21.49641299]]\n",
      "Difference:\n",
      "[[ 2.15358412  0.20328878  3.33508249  4.66392545  4.96218433]\n",
      " [ 2.71452886  0.76471902  3.89585684  5.2244843   5.52352927]\n",
      " [ 2.32959165  0.37946679  3.51240877 18.25058673 16.8731792 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"learned matrix:\")\n",
    "print(net.linearnetwork[0].weight.data.numpy())\n",
    "print(\"log(O):\")\n",
    "print(np.log(obs_mat))\n",
    "print(\"Difference:\")\n",
    "print(net.linearnetwork[0].weight.data.numpy() - np.log(obs_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned matrix:\n",
      "[[ 0.39010373  1.8261852  -2.6486025 ]\n",
      " [ 0.008606    0.00922457  1.2635484 ]\n",
      " [ 1.1878589   0.67104965  1.4508202 ]\n",
      " [ 3.0848243  -1.3024158   1.0486238 ]\n",
      " [-1.1736768   3.598973   -3.2347605 ]]\n",
      "pseudo inv of O:\n",
      "[[ 5.33398346e-01  2.62833064e+00 -1.97029301e+00]\n",
      " [-9.48583761e-03 -2.97974301e-03  1.06001933e+00]\n",
      " [ 9.08540466e-02 -4.56127065e-01  3.45443693e-01]\n",
      " [ 1.96231049e+00 -1.81806331e+00  7.42823603e-01]\n",
      " [-7.82054374e-01  4.31680259e+00 -2.80659284e+00]]\n",
      "Difference:\n",
      "[[-0.14329462 -0.80214541 -0.67830948]\n",
      " [ 0.01809184  0.01220431  0.20352905]\n",
      " [ 1.09700489  1.12717672  1.10537651]\n",
      " [ 1.12251384  0.51564747  0.3058002 ]\n",
      " [-0.39162247 -0.71782955 -0.42816768]]\n"
     ]
    }
   ],
   "source": [
    "print(\"learned matrix:\")\n",
    "print(net.linearnetwork_h[0].weight.data.numpy().T)\n",
    "print(\"pseudo inv of O:\")\n",
    "print(np.linalg.pinv(obs_mat))\n",
    "print(\"Difference:\")\n",
    "print(net.linearnetwork_h[0].weight.data.numpy().T - np.linalg.pinv(obs_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (linearnetwork): Sequential(\n",
       "    (0): Linear(in_features=5, out_features=3, bias=False)\n",
       "  )\n",
       "  (linearnetwork_h): Sequential(\n",
       "    (0): Linear(in_features=5, out_features=3, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transition matrix:\n",
      "[[0.19038151 0.5603291  0.24928939]\n",
      " [0.59211519 0.37810341 0.0297814 ]\n",
      " [0.52737761 0.34205379 0.1305686 ]]\n",
      "observation matrix:\n",
      "[[2.84422161e-01 2.51497236e-01 1.31816210e-02 4.38026768e-01\n",
      "  1.28722143e-02]\n",
      " [1.50940815e-01 6.59969385e-01 1.64551320e-02 2.15946003e-02\n",
      "  1.51040067e-01]\n",
      " [9.01408117e-03 9.45219115e-01 4.57668030e-02 7.87716140e-11\n",
      "  4.61558184e-10]]\n",
      "stationary distribution:\n",
      "[0.41619456 0.44908825 0.1347172 ]\n",
      "states and observations, first half of each row is states, only showing first 5:\n",
      "[[0. 1. 0. 0. 2. 1. 0. 2. 0. 2. 0. 0. 0. 0. 1. 0. 3. 1. 1. 1.]\n",
      " [0. 2. 0. 1. 1. 0. 2. 1. 0. 1. 3. 1. 3. 4. 1. 3. 1. 4. 0. 1.]\n",
      " [1. 0. 1. 0. 2. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      " [2. 0. 0. 1. 0. 1. 0. 2. 2. 1. 1. 1. 1. 1. 1. 4. 3. 1. 1. 0.]\n",
      " [0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 3. 1. 1. 1. 1. 1. 0. 1. 1. 1.]]\n",
      "positions, only showing first 5:  [2 2 5 8 1]\n",
      "Pr[H_i|x_-i], j-th row is for j-th sample and i=positions[j], only showing first 5:\n",
      "[[0.2431274  0.58828537 0.16858724]\n",
      " [0.59621374 0.33723514 0.06655112]\n",
      " [0.34529973 0.53576977 0.11893049]\n",
      " [0.42479842 0.48089777 0.0943038 ]\n",
      " [0.26754286 0.49347438 0.23898276]]\n",
      "Pr[X_i|x_-i], j-th row is for j-th sample and i=positions[j], only showing first 5:\n",
      "[[0.15946675 0.60874808 0.02060083 0.1192001  0.09198425]\n",
      " [0.22107884 0.43541637 0.01645414 0.26844003 0.05861061]\n",
      " [0.18015247 0.55284895 0.01881084 0.16282026 0.08536747]\n",
      " [0.19425925 0.51335119 0.01782875 0.19645788 0.07810293]\n",
      " [0.15273475 0.61885535 0.02258431 0.12784731 0.07797827]]\n",
      "Pr[H_i|x_i,x_-i], j-th row is for j-th sample and i=positions[j], only showing first 5:\n",
      "[[4.33637854e-01 5.56832517e-01 9.52962935e-03]\n",
      " [9.72871192e-01 2.71288079e-02 1.95289028e-11]\n",
      " [5.45154307e-01 4.48894905e-01 5.95078777e-03]\n",
      " [2.08114115e-01 6.18246942e-01 1.73638943e-01]\n",
      " [1.08727006e-01 5.26258660e-01 3.65014334e-01]]\n"
     ]
    }
   ],
   "source": [
    "states, obs = generate_HMM_sequences(trans_mat, obs_mat, stat_dist, length, num_samples) # generate sample sequences\n",
    "\n",
    "pos = np.random.randint(length, size = num_samples)\n",
    "\n",
    "print(\"transition matrix:\")\n",
    "print(trans_mat)\n",
    "print(\"observation matrix:\")\n",
    "print(obs_mat)\n",
    "print(\"stationary distribution:\")\n",
    "print(stat_dist)\n",
    "print(\"states and observations, first half of each row is states, only showing first 5:\")\n",
    "print(np.concatenate((states, obs), axis = 1)[:5])\n",
    "print(\"positions, only showing first 5: \", pos[:5])\n",
    "h, x, hh = x_i_conditional_prob(trans_mat, obs_mat, stat_dist, obs, pos)\n",
    "print(\"Pr[H_i|x_-i], j-th row is for j-th sample and i=positions[j], only showing first 5:\")\n",
    "print(h[:5])\n",
    "print(\"Pr[X_i|x_-i], j-th row is for j-th sample and i=positions[j], only showing first 5:\")\n",
    "print(x[:5])\n",
    "print(\"Pr[H_i|x_i,x_-i], j-th row is for j-th sample and i=positions[j], only showing first 5:\")\n",
    "print(hh[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_one_hot = np.zeros((num_samples, num_obs))\n",
    "for i in range(num_samples):\n",
    "    x_one_hot[i, int(obs[i, pos[i]])] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "features1, features2, labels = x, x_one_hot, hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = data.TensorDataset(torch.FloatTensor(features1), torch.FloatTensor(features2), torch.FloatTensor(labels))\n",
    "test_dl = torch.utils.data.DataLoader(test_dataset, batch_size=num_samples, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8436e-05, grad_fn=<DivBackward0>)\n",
      "tensor([[4.3322e-01, 5.5728e-01, 9.5045e-03],\n",
      "        [9.7290e-01, 2.7086e-02, 1.2877e-05],\n",
      "        [5.4537e-01, 4.4871e-01, 5.9128e-03],\n",
      "        [2.0881e-01, 6.1887e-01, 1.7232e-01],\n",
      "        [1.0937e-01, 5.2782e-01, 3.6280e-01]], grad_fn=<SliceBackward>)\n",
      "tensor([[4.3364e-01, 5.5683e-01, 9.5296e-03],\n",
      "        [9.7287e-01, 2.7129e-02, 1.9529e-11],\n",
      "        [5.4515e-01, 4.4889e-01, 5.9508e-03],\n",
      "        [2.0811e-01, 6.1825e-01, 1.7364e-01],\n",
      "        [1.0873e-01, 5.2626e-01, 3.6501e-01]])\n"
     ]
    }
   ],
   "source": [
    "for X1, X2, y in test_dl:\n",
    "    print(loss(torch.log(net(X1, X2)), y))\n",
    "    print(net(X1, X2)[:5])\n",
    "    print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
