{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Try_Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OABEbFjnKnC"
      },
      "source": [
        "import torch\n",
        "from torch.nn import TransformerEncoderLayer\n",
        "from torch import nn, Tensor\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import math"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qHF92sWDYdx"
      },
      "source": [
        "# Data parameters\n",
        "\n",
        "num_hidden_state = 3 # number of hidden states\n",
        "num_obs = 5          # number of possible observations\n",
        "seq_length = 10      # sequence length\n",
        "nsamples = 1000      # number of samples we want to generate\n",
        "\n",
        "# Set model parameters\n",
        "emsize = 200         # embedding dimension/feature dimension\n",
        "d_hid = 2048         # dimension of the feedforward network in TransformerEncoder\n",
        "nhead = 2            # number of heads in multi-head attention\n",
        "ntoken = num_obs + 1 # vocabulary size\n",
        "batch_size = 200     # batch size \n",
        "lr = 1e-3            # learning rate\n",
        "epochs = 200         # number of training epochs"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmwRZ_OxjNJt"
      },
      "source": [
        "# Generate HMM parameters\n",
        "def generate_HMM_params(num_hidden_state, num_obs):\n",
        "    # random generate the transition matrix and observation matrix, and compute the stationary distribution\n",
        "    \n",
        "    alpha_state = np.ones(num_hidden_state)\n",
        "    alpha_obs = np.ones(num_obs) / num_obs\n",
        "    trans_mat = np.random.dirichlet(alpha_state, num_hidden_state)\n",
        "    obs_mat = np.random.dirichlet(alpha_obs, num_hidden_state)\n",
        "    tmp = np.ones((num_hidden_state + 1, num_hidden_state))\n",
        "    tmp[:-1] = np.identity(num_hidden_state) - trans_mat.T\n",
        "    tmp_v = np.zeros(num_hidden_state + 1)\n",
        "    tmp_v[-1] = 1\n",
        "    stat_dist = np.linalg.lstsq(tmp, tmp_v, rcond=None)[0]\n",
        "    return trans_mat, obs_mat, stat_dist"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWO0xfEKjSuT"
      },
      "source": [
        "# Sample HMM sequences\n",
        "def generate_HMM_sequences(trans_mat, obs_mat, init_dist, length, num_samples = 1):\n",
        "    # generate sample sequences from HMM using the parameters given\n",
        "    \n",
        "    states = np.zeros((num_samples, length))\n",
        "    obs = np.zeros((num_samples, length))\n",
        "    tmp_state = np.argmax(np.random.multinomial(1, init_dist, num_samples), axis = 1)\n",
        "    #print(tmp_state)\n",
        "    for i in range(length):\n",
        "        #print(\"i: \", i)\n",
        "        states[:, i] = tmp_state\n",
        "        for j in range(num_samples):\n",
        "            obs[j, i] = np.random.multinomial(1, obs_mat[tmp_state[j]]).argmax()\n",
        "            tmp_state[j] = np.random.multinomial(1, trans_mat[tmp_state[j]]).argmax()\n",
        "        #print(\"obs[:, i]: \", obs[:, i])\n",
        "    return states, obs"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RDCKvMtj_8c"
      },
      "source": [
        "# Add [mask] tokens to input, one per sequence\n",
        "def add_mask_to_sequences(seqs, pos):\n",
        "  masked_seqs = np.copy(seqs)\n",
        "  for i in range(nsamples):\n",
        "    masked_seqs[i, pos[i]] = num_obs\n",
        "  return masked_seqs"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL_XeUASnt24"
      },
      "source": [
        "# Define Transformer Model\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "  def __init__(self, emsize: int, nhead: int, ntoken: int):\n",
        "    super().__init__()\n",
        "    self.emsize = emsize\n",
        "    self.encoder = nn.Embedding(ntoken, emsize)\n",
        "    #self.pos_encoder = PositionalEncoding(emsize, dropout)\n",
        "    self.transformer_encoder = TransformerEncoderLayer(emsize, nhead, d_hid, batch_first=True)\n",
        "    self.decoder = nn.Linear(emsize, ntoken)\n",
        "  \n",
        "  def forward(self, src: Tensor) -> Tensor:\n",
        "    # original input: (batch_size, seq_length)\n",
        "    #print(src.shape)\n",
        "    src = self.encoder(src) * math.sqrt(self.emsize)\n",
        "    # after embedding: (batch_size, seq_length, emsize)\n",
        "    #src = self.pos_encoder(src)\n",
        "    #print(src.shape)\n",
        "    output = self.transformer_encoder(src)\n",
        "    #print(output.shape)\n",
        "    # after encoder: (batch_size, seq_length, emsize)\n",
        "    output = self.decoder(output)\n",
        "    # after decoder: (batch_size, seq_length, ntoken)\n",
        "    return output"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPHOEdVC4v4Z"
      },
      "source": [
        "# Generate HMM parameters and samples used for training\n",
        "seed = 20211121\n",
        "np.random.seed(seed)\n",
        "trans_mat, obs_mat, stat_dist = generate_HMM_params(num_hidden_state, num_obs) # generate parameters for HMM\n",
        "states, obs = generate_HMM_sequences(trans_mat, obs_mat, stat_dist, seq_length, nsamples) # generate training sequences\n",
        "pos = np.random.randint(seq_length, size = nsamples) # positions for masks, nsamples-dimensional array\n",
        "masked_obs = add_mask_to_sequences(obs, pos)\n",
        "val_states, val_obs = generate_HMM_sequences(trans_mat, obs_mat, stat_dist, seq_length, nsamples) # generate validation sequences\n",
        "val_pos = np.random.randint(seq_length, size = nsamples)\n",
        "val_masked_obs = add_mask_to_sequences(val_obs, val_pos)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_aUf4Z-tSGu"
      },
      "source": [
        "# Prepare input data and validation data\n",
        "dataset = torch.utils.data.TensorDataset(torch.LongTensor(masked_obs), torch.LongTensor(obs))\n",
        "val_dataset = torch.utils.data.TensorDataset(torch.LongTensor(val_masked_obs), torch.LongTensor(val_obs))\n",
        "train_dl = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "val_dl = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lU7mXAWD9jn"
      },
      "source": [
        "# Set up model instance\n",
        "model = TransformerModel(emsize, nhead, ntoken)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNrQcy5IEXVc"
      },
      "source": [
        "# Set up optimizer and loss\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyO-gCkjGHDT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83197239-382f-4fdf-ddc9-f2a5742c74fd"
      },
      "source": [
        "# Training process\n",
        "model.train()\n",
        "for i in range(epochs):\n",
        "  total_loss = 0.\n",
        "  for data, target in train_dl:\n",
        "    #data = data[0]\n",
        "    output = model(data)\n",
        "    loss = criterion(output.transpose(1, 2), target)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss += loss.item()\n",
        "  if i % 10 == 0:\n",
        "    print(\"epoch \" + str(i))\n",
        "    print(\"training loss: \" + str(total_loss))\n",
        "    model.eval()\n",
        "    total_val_loss = 0.\n",
        "    for val_data, val_target in val_dl:\n",
        "      #val_data = val_data[0]\n",
        "      val_output = model(val_data)\n",
        "      val_loss = criterion(val_output.transpose(1, 2), val_target)\n",
        "      total_val_loss += val_loss.item()\n",
        "    print(\"val loss: \" + str(total_val_loss))\n",
        "    model.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0\n",
            "training loss: 8.119575500488281\n",
            "val loss: 6.989755749702454\n",
            "epoch 10\n",
            "training loss: 2.186689078807831\n",
            "val loss: 1.9896335303783417\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeZWYTaSHa-p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f333e864-32ee-42fa-a74d-ee4b9332c087"
      },
      "source": [
        "train_err = 0\n",
        "val_err = 0\n",
        "model.eval()\n",
        "for data, target in train_dl:\n",
        "  #data = data[0]\n",
        "  #print(data[:5])\n",
        "  output = model(data) # (batch_size, seq_length, ntoken)\n",
        "  #print(torch.argmax(output, dim=2)[:5])\n",
        "  train_err += torch.sum(torch.argmax(output, dim=2) != target)\n",
        "  #print(output[:5])\n",
        "  #print(target[:5])\n",
        "  #print(output.shape)\n",
        "  #print(data)\n",
        "  #print(data.shape)\n",
        "  #print(output.transpose(1, 2))\n",
        "  #print(output.transpose(1, 2).shape)\n",
        "  #loss = criterion(output.transpose(1, 2), target) # CrossEntropyLoss takes input of size (N, C, d) and (N, d) where N: number of data, C: number of classes, d: extra dim, so need to swap the dimension of output from (batch_size, seq_length, ntoken) to (batch_size, ntoken, seq_length)\n",
        "  #print(loss)\n",
        "for val_data, val_target in val_dl:\n",
        "  #val_data = val_data[0]\n",
        "  #print(val_data[:5])\n",
        "  val_output = model(val_data)\n",
        "  val_err += torch.sum(torch.argmax(val_output, dim=2) != val_target)\n",
        "  #print(val_output[:5])\n",
        "  #print(val_target[:5])\n",
        "  #val_loss = criterion(val_output.transpose(1, 2), val_target)\n",
        "  #print(val_loss)\n",
        "print(train_err)\n",
        "print(val_err)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(4834)\n",
            "tensor(4898)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi2kJ3ZamPSJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}