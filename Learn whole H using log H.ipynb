{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameter setting\n",
    "num_hidden_state = 3\n",
    "num_obs = 5\n",
    "length = 10\n",
    "num_samples = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define some useful functions\n",
    "def generate_HMM_params(num_hidden_state, num_obs):\n",
    "    # random generate the transition matrix and observation matrix, and compute the stationary distribution\n",
    "    \n",
    "    alpha_state = np.ones(num_hidden_state)\n",
    "    alpha_obs = np.ones(num_obs) / num_obs\n",
    "    trans_mat = np.random.dirichlet(alpha_state, num_hidden_state)\n",
    "    obs_mat = np.random.dirichlet(alpha_obs, num_hidden_state)\n",
    "    tmp = np.ones((num_hidden_state + 1, num_hidden_state))\n",
    "    tmp[:-1] = np.identity(num_hidden_state) - trans_mat.T\n",
    "    tmp_v = np.zeros(num_hidden_state + 1)\n",
    "    tmp_v[-1] = 1\n",
    "    stat_dist = np.linalg.lstsq(tmp, tmp_v, rcond=None)[0]\n",
    "    return trans_mat, obs_mat, stat_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_HMM_sequences(trans_mat, obs_mat, init_dist, length, num_samples = 1):\n",
    "    # generate sample sequences from HMM\n",
    "    \n",
    "    states = np.zeros((num_samples, length))\n",
    "    obs = np.zeros((num_samples, length))\n",
    "    tmp_state = np.argmax(np.random.multinomial(1, init_dist, num_samples), axis = 1)\n",
    "    #print(tmp_state)\n",
    "    for i in range(length):\n",
    "        #print(\"i: \", i)\n",
    "        states[:, i] = tmp_state\n",
    "        for j in range(num_samples):\n",
    "            obs[j, i] = np.random.multinomial(1, obs_mat[tmp_state[j]]).argmax()\n",
    "            tmp_state[j] = np.random.multinomial(1, trans_mat[tmp_state[j]]).argmax()\n",
    "        #print(\"obs[:, i]: \", obs[:, i])\n",
    "    return states, obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_compute(trans_mat, obs_mat, init_dist, obs_to_pos):\n",
    "    # compute \\sum_{h_1,...,h_{pos-1}} P(h_1,...,h_{pos},x_1,...,x_{pos-1})\n",
    "    pos = obs_to_pos.shape[0] + 1\n",
    "    num_hidden_state = trans_mat.shape[0]\n",
    "    num_obs = obs_mat.shape[1]\n",
    "    forward = np.zeros((pos, num_hidden_state))\n",
    "    forward[0] = init_dist\n",
    "    for i in range(1, pos):\n",
    "        for j in range(num_hidden_state):\n",
    "            for k in range(num_hidden_state):\n",
    "                #print(i, j, k)\n",
    "                #print(forward[i - 1, k], trans_mat[k, j], obs_mat[k, int(obs_to_pos[i - 1])])\n",
    "                forward[i, j] += forward[i - 1, k] * trans_mat[k, j] * obs_mat[k, int(obs_to_pos[i - 1])]\n",
    "    #print(\"forward: \", forward)\n",
    "    return forward[pos - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_compute(trans_mat, obs_mat, obs_from_pos):\n",
    "    num_hidden_state = trans_mat.shape[0]\n",
    "    num_obs = obs_mat.shape[1]\n",
    "    back_length = obs_from_pos.shape[0]\n",
    "    if (back_length == 0):\n",
    "        return np.ones(num_hidden_state)\n",
    "    backward = np.zeros((back_length, num_hidden_state))\n",
    "    for j in range(num_hidden_state):\n",
    "         for k in range(num_hidden_state):\n",
    "            backward[0, j] += trans_mat[j, k] * obs_mat[k, int(obs_from_pos[-1])]\n",
    "    for i in range(1, back_length):\n",
    "        for j in range(num_hidden_state):\n",
    "            for k in range(num_hidden_state):\n",
    "                backward[i, j] += trans_mat[j, k] * obs_mat[k, int(obs_from_pos[-(i + 1)])] * backward[i - 1, k]\n",
    "    #print(\"backward: \", backward)\n",
    "    return backward[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_i_conditional_prob(trans_mat, obs_mat, init_dist, known_X, pos):\n",
    "    num_hidden_state = trans_mat.shape[0]\n",
    "    num_obs = obs_mat.shape[1]\n",
    "    num_samples = known_X.shape[0]\n",
    "    length = known_X.shape[1]\n",
    "    x_pos_conditional_prob = np.zeros((num_samples, num_obs))\n",
    "    h_pos_conditional_prob = np.zeros((num_samples, num_hidden_state))\n",
    "    h_all_pos_conditional_prob = np.zeros((num_samples, num_hidden_state))\n",
    "    for i in range(num_samples):\n",
    "        #print(\"x_i_conditional_prob: i=\", i)\n",
    "        sample_obs_vec = known_X[i]\n",
    "        forward_vec = forward_compute(trans_mat, obs_mat, init_dist, known_X[i, :pos[i]])\n",
    "        backward_vec = backward_compute(trans_mat, obs_mat, known_X[i, pos[i] + 1:])\n",
    "        #print(\"forward_vec: \", forward_vec)\n",
    "        #print(\"backward_vec: \", backward_vec)\n",
    "        h_prob_tmp = forward_vec * backward_vec\n",
    "        tmp = h_prob_tmp.sum()\n",
    "        h_prob_tmp /= tmp\n",
    "        h_pos_conditional_prob[i] = h_prob_tmp\n",
    "        x_pos_conditional_prob[i] = h_prob_tmp @ obs_mat\n",
    "        h_all_pos_conditional_prob[i] = h_prob_tmp * obs_mat[:, int(known_X[i, pos[i]])] / x_pos_conditional_prob[i, int(known_X[i, pos[i]])]\n",
    "    return h_pos_conditional_prob, x_pos_conditional_prob, h_all_pos_conditional_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transition matrix\n",
      "[[0.19038151 0.5603291  0.24928939]\n",
      " [0.59211519 0.37810341 0.0297814 ]\n",
      " [0.52737761 0.34205379 0.1305686 ]]\n",
      "observation matrix\n",
      "[[2.84422161e-01 2.51497236e-01 1.31816210e-02 4.38026768e-01\n",
      "  1.28722143e-02]\n",
      " [1.50940815e-01 6.59969385e-01 1.64551320e-02 2.15946003e-02\n",
      "  1.51040067e-01]\n",
      " [9.01408117e-03 9.45219115e-01 4.57668030e-02 7.87716140e-11\n",
      "  4.61558184e-10]]\n",
      "stationary distribution\n",
      "[0.41619456 0.44908825 0.1347172 ]\n",
      "states and observations, first half of each row is states, only showing first 5: \n",
      "[[1. 1. 0. 0. 0. 2. 0. 1. 0. 1. 0. 1. 0. 3. 3. 1. 1. 0. 3. 1.]\n",
      " [1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 4. 3. 3. 4. 1. 0. 4. 1. 1.]\n",
      " [0. 2. 0. 2. 0. 2. 2. 2. 1. 0. 3. 1. 1. 1. 0. 1. 1. 1. 1. 1.]\n",
      " [0. 2. 2. 0. 1. 0. 2. 1. 2. 0. 1. 1. 1. 3. 1. 0. 1. 1. 1. 2.]\n",
      " [1. 0. 1. 0. 2. 1. 1. 1. 0. 1. 4. 1. 4. 1. 1. 0. 1. 0. 3. 3.]]\n",
      "positions, only showing first 5:  [9 4 6 2 1]\n",
      "Pr[H_i|x_-i], j-th row is for j-th sample and i=positions[j], only showing first 5:\n",
      "[[0.21892801 0.54738046 0.23369153]\n",
      " [0.33619463 0.45004706 0.21375831]\n",
      " [0.60101586 0.31777197 0.08121217]\n",
      " [0.24157682 0.61921196 0.13921122]\n",
      " [0.62919903 0.33535031 0.03545066]]\n",
      "Pr[X_i|x_-i], j-th row is for j-th sample and i=positions[j], only showing first 5:\n",
      "[[0.14699655 0.63720383 0.02258836 0.10771679 0.08549447]\n",
      " [0.16547851 0.58361774 0.02162021 0.15698083 0.07230271]\n",
      " [0.21963904 0.43763689 0.01686816 0.2701232  0.0557327 ]\n",
      " [0.16342902 0.60100195 0.01974484 0.11918875 0.09663544]\n",
      " [0.22989575 0.4130714  0.01543456 0.28284777 0.05875052]]\n",
      "Pr[H_i|x_i,x_-i], j-th row is for j-th sample and i=positions[j], only showing first 5:\n",
      "[[8.64084416e-02 5.66936866e-01 3.46654692e-01]\n",
      " [5.98534890e-02 9.40146510e-01 1.36456714e-09]\n",
      " [3.45386395e-01 4.79209535e-01 1.75404070e-01]\n",
      " [1.01091024e-01 6.79966078e-01 2.18942898e-01]\n",
      " [3.83085874e-01 5.35793431e-01 8.11206949e-02]]\n"
     ]
    }
   ],
   "source": [
    "seed = 20211018\n",
    "np.random.seed(seed)\n",
    "trans_mat, obs_mat, stat_dist = generate_HMM_params(num_hidden_state, num_obs) # generate parameters for HMM\n",
    "\n",
    "states, obs = generate_HMM_sequences(trans_mat, obs_mat, stat_dist, length, num_samples) # generate sample sequences\n",
    "\n",
    "pos = np.random.randint(length, size = num_samples)\n",
    "\n",
    "print(\"transition matrix\")\n",
    "print(trans_mat)\n",
    "print(\"observation matrix\")\n",
    "print(obs_mat)\n",
    "print(\"stationary distribution\")\n",
    "print(stat_dist)\n",
    "print(\"states and observations, first half of each row is states, only showing first 5: \")\n",
    "print(np.concatenate((states, obs), axis = 1)[:5])\n",
    "print(\"positions, only showing first 5: \", pos[:5])\n",
    "h, x, hh = x_i_conditional_prob(trans_mat, obs_mat, stat_dist, obs, pos)\n",
    "print(\"Pr[H_i|x_-i], j-th row is for j-th sample and i=positions[j], only showing first 5:\")\n",
    "print(h[:5])\n",
    "print(\"Pr[X_i|x_-i], j-th row is for j-th sample and i=positions[j], only showing first 5:\")\n",
    "print(x[:5])\n",
    "print(\"Pr[H_i|x_i,x_-i], j-th row is for j-th sample and i=positions[j], only showing first 5:\")\n",
    "print(hh[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "logh = np.log(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linearnetwork = nn.Sequential(\n",
    "            nn.Linear(num_obs, num_hidden_state, bias=False)\n",
    "        )\n",
    "    \n",
    "    def forward(self, logh, ind_x):\n",
    "        logits = self.linearnetwork(ind_x)\n",
    "        return nn.Softmax(dim = 1)(logh + logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 4 6 2 1 3 1 2 1]\n",
      "[[0. 1. 0. 3. 3. 1. 1. 0. 3. 1.]\n",
      " [1. 4. 3. 3. 4. 1. 0. 4. 1. 1.]\n",
      " [3. 1. 1. 1. 0. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 3. 1. 0. 1. 1. 1. 2.]\n",
      " [4. 1. 4. 1. 1. 0. 1. 0. 3. 3.]\n",
      " [1. 0. 1. 0. 0. 3. 1. 0. 3. 1.]\n",
      " [0. 0. 1. 1. 1. 1. 1. 1. 1. 3.]\n",
      " [4. 3. 0. 1. 1. 1. 0. 3. 1. 3.]\n",
      " [3. 1. 0. 2. 4. 3. 1. 3. 1. 1.]]\n",
      "[[0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "x_one_hot = np.zeros((num_samples, num_obs))\n",
    "for i in range(num_samples):\n",
    "    x_one_hot[i, int(obs[i, pos[i]])] = 1\n",
    "#print(pos[:9])\n",
    "#print(obs[:9])\n",
    "#print(x_one_hot[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features1, features2, labels = logh, x_one_hot, hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters.\n",
    "lr = 1\n",
    "epochs = 1000\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.TensorDataset(torch.FloatTensor(features1), torch.FloatTensor(features2), torch.FloatTensor(labels))\n",
    "train_dl = data.DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.KLDivLoss(reduction='batchmean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shiftedNegLLLoss(outputs, targets):\n",
    "    batchsize = outputs.size()[0]\n",
    "    log_outputs = torch.log(outputs)\n",
    "    tmp_loss = - targets * log_outputs\n",
    "    tmp = - targets * torch.log(targets)\n",
    "    return torch.sum(tmp_loss - tmp) / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = torch.optim.SGD(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "loss:  tensor(1.7009, grad_fn=<AddBackward0>)\n",
      "epoch:  100\n",
      "loss:  tensor(0.0122, grad_fn=<AddBackward0>)\n",
      "epoch:  200\n",
      "loss:  tensor(0.0064, grad_fn=<AddBackward0>)\n",
      "epoch:  300\n",
      "loss:  tensor(0.0043, grad_fn=<AddBackward0>)\n",
      "epoch:  400\n",
      "loss:  tensor(0.0033, grad_fn=<AddBackward0>)\n",
      "epoch:  500\n",
      "loss:  tensor(0.0026, grad_fn=<AddBackward0>)\n",
      "epoch:  600\n",
      "loss:  tensor(0.0022, grad_fn=<AddBackward0>)\n",
      "epoch:  700\n",
      "loss:  tensor(0.0019, grad_fn=<AddBackward0>)\n",
      "epoch:  800\n",
      "loss:  tensor(0.0017, grad_fn=<AddBackward0>)\n",
      "epoch:  900\n",
      "loss:  tensor(0.0015, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "total_loss_lst = []\n",
    "for i in range(epochs):\n",
    "    total_loss = 0\n",
    "    for X1, X2, y in train_dl:\n",
    "        l = loss(torch.log(net(X1, X2)), y)\n",
    "        #l = shiftedNegLLLoss(net(X1, X2), y)\n",
    "        total_loss += l\n",
    "        trainer.zero_grad()\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "    if (i % 100 == 0):\n",
    "        print(\"epoch: \", i)\n",
    "        print(\"loss: \", total_loss)\n",
    "        total_loss_lst.append(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x_lst = list(range(0, 1000, 100))\n",
    "plt.plot(x_lst, total_loss_lst)\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"training loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned matrix:\n",
      "[[ 1.3045962  -0.62654877 -0.42787576  3.3971775   0.25236118]\n",
      " [ 0.6710247   0.33821243 -0.20606275  0.38431484  2.7212012 ]\n",
      " [-2.1470218   0.69743556  0.8168571  -3.3849497  -2.8997552 ]]\n",
      "log(O):\n",
      "[[ -1.25729566  -1.38032328  -4.32893177  -0.82547526  -4.35268422]\n",
      " [ -1.89086747  -0.41556183  -4.10711787  -3.83531198  -1.89021013]\n",
      " [ -4.70896735  -0.05633851  -3.08419628 -23.26446841 -21.49641299]]\n",
      "Difference:\n",
      "[[ 2.56189185  0.75377451  3.90105601  4.22265271  4.6050454 ]\n",
      " [ 2.56189215  0.75377426  3.90105512  4.21962682  4.61141132]\n",
      " [ 2.56194558  0.75377407  3.90105338 19.87951873 18.59665775]]\n"
     ]
    }
   ],
   "source": [
    "print(\"learned matrix:\")\n",
    "print(net.linearnetwork[0].weight.data.numpy())\n",
    "print(\"log(O):\")\n",
    "print(np.log(obs_mat))\n",
    "print(\"Difference:\")\n",
    "print(net.linearnetwork[0].weight.data.numpy() - np.log(obs_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (linearnetwork): Sequential(\n",
       "    (0): Linear(in_features=5, out_features=3, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transition matrix:\n",
      "[[0.19038151 0.5603291  0.24928939]\n",
      " [0.59211519 0.37810341 0.0297814 ]\n",
      " [0.52737761 0.34205379 0.1305686 ]]\n",
      "observation matrix:\n",
      "[[2.84422161e-01 2.51497236e-01 1.31816210e-02 4.38026768e-01\n",
      "  1.28722143e-02]\n",
      " [1.50940815e-01 6.59969385e-01 1.64551320e-02 2.15946003e-02\n",
      "  1.51040067e-01]\n",
      " [9.01408117e-03 9.45219115e-01 4.57668030e-02 7.87716140e-11\n",
      "  4.61558184e-10]]\n",
      "stationary distribution:\n",
      "[0.41619456 0.44908825 0.1347172 ]\n",
      "states and observations, first half of each row is states, only showing first 5:\n",
      "[[0. 1. 0. 0. 2. 1. 0. 2. 0. 2. 0. 0. 0. 0. 1. 0. 3. 1. 1. 1.]\n",
      " [0. 2. 0. 1. 1. 0. 2. 1. 0. 1. 3. 1. 3. 4. 1. 3. 1. 4. 0. 1.]\n",
      " [1. 0. 1. 0. 2. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      " [2. 0. 0. 1. 0. 1. 0. 2. 2. 1. 1. 1. 1. 1. 1. 4. 3. 1. 1. 0.]\n",
      " [0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 3. 1. 1. 1. 1. 1. 0. 1. 1. 1.]]\n",
      "positions, only showing first 5:  [2 2 5 8 1]\n",
      "Pr[H_i|x_-i], j-th row is for j-th sample and i=positions[j], only showing first 5:\n",
      "[[0.2431274  0.58828537 0.16858724]\n",
      " [0.59621374 0.33723514 0.06655112]\n",
      " [0.34529973 0.53576977 0.11893049]\n",
      " [0.42479842 0.48089777 0.0943038 ]\n",
      " [0.26754286 0.49347438 0.23898276]]\n",
      "Pr[X_i|x_-i], j-th row is for j-th sample and i=positions[j], only showing first 5:\n",
      "[[0.15946675 0.60874808 0.02060083 0.1192001  0.09198425]\n",
      " [0.22107884 0.43541637 0.01645414 0.26844003 0.05861061]\n",
      " [0.18015247 0.55284895 0.01881084 0.16282026 0.08536747]\n",
      " [0.19425925 0.51335119 0.01782875 0.19645788 0.07810293]\n",
      " [0.15273475 0.61885535 0.02258431 0.12784731 0.07797827]]\n",
      "Pr[H_i|x_i,x_-i], j-th row is for j-th sample and i=positions[j], only showing first 5:\n",
      "[[4.33637854e-01 5.56832517e-01 9.52962935e-03]\n",
      " [9.72871192e-01 2.71288079e-02 1.95289028e-11]\n",
      " [5.45154307e-01 4.48894905e-01 5.95078777e-03]\n",
      " [2.08114115e-01 6.18246942e-01 1.73638943e-01]\n",
      " [1.08727006e-01 5.26258660e-01 3.65014334e-01]]\n"
     ]
    }
   ],
   "source": [
    "states, obs = generate_HMM_sequences(trans_mat, obs_mat, stat_dist, length, num_samples) # generate sample sequences\n",
    "\n",
    "pos = np.random.randint(length, size = num_samples)\n",
    "\n",
    "print(\"transition matrix:\")\n",
    "print(trans_mat)\n",
    "print(\"observation matrix:\")\n",
    "print(obs_mat)\n",
    "print(\"stationary distribution:\")\n",
    "print(stat_dist)\n",
    "print(\"states and observations, first half of each row is states, only showing first 5:\")\n",
    "print(np.concatenate((states, obs), axis = 1)[:5])\n",
    "print(\"positions, only showing first 5: \", pos[:5])\n",
    "h, x, hh = x_i_conditional_prob(trans_mat, obs_mat, stat_dist, obs, pos)\n",
    "print(\"Pr[H_i|x_-i], j-th row is for j-th sample and i=positions[j], only showing first 5:\")\n",
    "print(h[:5])\n",
    "print(\"Pr[X_i|x_-i], j-th row is for j-th sample and i=positions[j], only showing first 5:\")\n",
    "print(x[:5])\n",
    "print(\"Pr[H_i|x_i,x_-i], j-th row is for j-th sample and i=positions[j], only showing first 5:\")\n",
    "print(hh[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 5 8 1 9 4 2 4]\n",
      "[[0. 0. 0. 0. 1. 0. 3. 1. 1. 1.]\n",
      " [3. 1. 3. 4. 1. 3. 1. 4. 0. 1.]\n",
      " [1. 0. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 4. 3. 1. 1. 0.]\n",
      " [3. 1. 1. 1. 1. 1. 0. 1. 1. 1.]\n",
      " [1. 0. 1. 0. 1. 1. 1. 1. 3. 0.]\n",
      " [1. 3. 1. 3. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 0. 1. 3. 1. 3. 1. 3. 4.]\n",
      " [1. 1. 0. 1. 1. 3. 0. 1. 3. 1.]]\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "logh = np.log(h)\n",
    "x_one_hot = np.zeros((num_samples, num_obs))\n",
    "for i in range(num_samples):\n",
    "    x_one_hot[i, int(obs[i, pos[i]])] = 1\n",
    "print(pos[:9])\n",
    "print(obs[:9])\n",
    "print(x_one_hot[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "features1, features2, labels = logh, x_one_hot, hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = data.TensorDataset(torch.FloatTensor(features1), torch.FloatTensor(features2), torch.FloatTensor(labels))\n",
    "test_dl = torch.utils.data.DataLoader(test_dataset, batch_size=num_samples, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0001, grad_fn=<DivBackward0>)\n",
      "1.9857181814941782e-07\n",
      "tensor([-1.0633, -0.6241, -2.1292])\n",
      "[-1.06421086 -0.62362112 -2.12863179]\n",
      "tensor([1., 0., 0., 0., 0.])\n",
      "tensor([0.5452, 0.4489, 0.0060])\n",
      "tensor([[4.3364e-01, 5.5683e-01, 9.5301e-03],\n",
      "        [9.7283e-01, 2.7046e-02, 1.2313e-04],\n",
      "        [5.4515e-01, 4.4889e-01, 5.9511e-03],\n",
      "        [2.0811e-01, 6.1825e-01, 1.7364e-01],\n",
      "        [1.0873e-01, 5.2626e-01, 3.6501e-01]], grad_fn=<SliceBackward>)\n",
      "[0.54515431 0.4488949  0.00595079]\n",
      "tensor([[4.3364e-01, 5.5683e-01, 9.5296e-03],\n",
      "        [9.7287e-01, 2.7129e-02, 1.9529e-11],\n",
      "        [5.4515e-01, 4.4889e-01, 5.9508e-03],\n",
      "        [2.0811e-01, 6.1825e-01, 1.7364e-01],\n",
      "        [1.0873e-01, 5.2626e-01, 3.6501e-01]])\n"
     ]
    }
   ],
   "source": [
    "for X1, X2, Y in test_dl:\n",
    "    print(loss(torch.log(net(X1, X2)), Y))\n",
    "    print(np.linalg.norm(net(X1, X2).detach().numpy() - Y.numpy()) ** 2 / num_samples)\n",
    "    print(X1[2])\n",
    "    print(np.log(np.array([0.345, 0.536, 0.119])))\n",
    "    print(X2[2])\n",
    "    print(Y[2])\n",
    "    print(net(X1, X2)[:5])\n",
    "    tmp = X2[2].numpy().T @ np.log(obs_mat.T) + X1[2].numpy()\n",
    "    print(np.exp(tmp) / np.sum(np.exp(tmp)))\n",
    "    print(Y[:5])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net.linearnetwork[0].weight.data = torch.from_numpy(np.log(obs_mat)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -1.2573,  -1.3803,  -4.3289,  -0.8255,  -4.3527],\n",
       "        [ -1.8909,  -0.4156,  -4.1071,  -3.8353,  -1.8902],\n",
       "        [ -4.7090,  -0.0563,  -3.0842, -23.2645, -21.4964]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.linearnetwork[0].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.5190, -0.6026, -1.4538])\n",
      "tensor([0., 1., 0., 0., 0.])\n",
      "tensor([0.0864, 0.5669, 0.3467], grad_fn=<SelectBackward>)\n",
      "tensor([0.0864, 0.5669, 0.3467])\n",
      "[[ -1.2572956   -1.3803233   -4.328932    -0.8254753   -4.352684  ]\n",
      " [ -1.8908675   -0.41556183  -4.1071177   -3.835312    -1.8902102 ]\n",
      " [ -4.708967    -0.05633851  -3.0841963  -23.26447    -21.496412  ]]\n",
      "[-2.89933561 -1.01817301 -1.51009174]\n",
      "[-2.8993356 -1.018173  -1.5100918]\n",
      "[ 1.02977817e-08 -5.67847230e-09  1.08131138e-09]\n",
      "[[ -1.25729566  -1.38032328  -4.32893177  -0.82547526  -4.35268422]\n",
      " [ -1.89086747  -0.41556183  -4.10711787  -3.83531198  -1.89021013]\n",
      " [ -4.70896735  -0.05633851  -3.08419628 -23.26446841 -21.49641299]]\n"
     ]
    }
   ],
   "source": [
    "for X1, X2, y in train_dl:\n",
    "    print(X1[0])\n",
    "    print(X2[0])\n",
    "    print(net(X1, X2)[0])\n",
    "    print(y[0])\n",
    "    print(net.linearnetwork[0].weight.data.numpy())\n",
    "    print(X2[2].numpy().T @ np.log(obs_mat.T) + X1[0].numpy())\n",
    "    print(X2[2].numpy().T @ net.linearnetwork[0].weight.data.numpy().T + X1[0].numpy())\n",
    "    print(X2[2].numpy().T @ (np.log(obs_mat.T) - net.linearnetwork[0].weight.data.numpy().T))\n",
    "    print(np.log(obs_mat))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transition matrix\n",
      "[[0.19038151 0.5603291  0.24928939]\n",
      " [0.59211519 0.37810341 0.0297814 ]\n",
      " [0.52737761 0.34205379 0.1305686 ]]\n",
      "observation matrix\n",
      "[[2.84422161e-01 2.51497236e-01 1.31816210e-02 4.38026768e-01\n",
      "  1.28722143e-02]\n",
      " [1.50940815e-01 6.59969385e-01 1.64551320e-02 2.15946003e-02\n",
      "  1.51040067e-01]\n",
      " [9.01408117e-03 9.45219115e-01 4.57668030e-02 7.87716140e-11\n",
      "  4.61558184e-10]]\n",
      "stationary distribution\n",
      "[0.41619456 0.44908825 0.1347172 ]\n",
      "states and observations, first half of each row is states\n",
      "[[1. 0. 1. ... 2. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [1. 1. 0. ... 1. 1. 0.]\n",
      " ...\n",
      " [0. 0. 1. ... 1. 1. 1.]\n",
      " [0. 1. 1. ... 1. 1. 1.]\n",
      " [1. 0. 1. ... 1. 0. 1.]]\n",
      "positions:  [8 3 0 2 1 5 1 6 6 2 2 6 5 5 1 4 7 7 8 6 0 8 5 9 0 2 8 7 1 9 7 9 1 8 2 4 1\n",
      " 1 4 7 6 2 7 3 9 2 7 6 2 5 8 7 8 9 0 1 8 0 0 9 5 4 7 6 7 3 4 9 5 8 3 7 8 3\n",
      " 9 9 0 4 2 6 6 9 1 9 1 5 5 8 1 4 0 3 1 4 7 1 0 6 9 2 2 2 7 4 8 2 7 2 2 6 1\n",
      " 4 7 6 8 0 1 4 2 1 8 7 3 1 7 8 3 1 7 6 3 5 1 9 1 5 8 3 1 7 7 0 8 6 7 4 3 1\n",
      " 8 4 8 8 7 8 1 9 2 1 9 1 7 5 6 7 7 5 4 2 8 6 8 5 2 5 1 1 8 3 8 4 0 4 6 2 4\n",
      " 5 5 7 0 4 9 8 7 8 6 5 3 8 5 5 1 2 1 5 4 4 2 2 6 2 6 5 1 7 5 7 1 1 9 1 5 2\n",
      " 8 5 0 3 6 9 9 1 6 4 7 7 0 1 6 1 4 0 5 5 1 6 2 9 8 8 3 5 1 0 8 2 5 0 4 2 1\n",
      " 1 0 7 0 2 8 0 4 0 3 4 6 9 9 2 4 1 3 0 3 8 1 3 5 3 2 4 0 1 7 9 2 2 3 3 3 8\n",
      " 0 2 1 1 5 1 9 8 9 8 7 7 6 7 2 1 1 8 4 9 0 7 1 5 6 7 7 5 8 9 3 3 4 6 0 0 4\n",
      " 5 8 4 3 1 5 3 0 3 5 9 2 1 6 2 0 9 2 6 5 7 0 6 1 0 0 6 5 0 6 0 1 7 5 5 9 7\n",
      " 2 2 5 0 2 8 7 5 0 9 9 6 8 9 1 2 9 9 7 8 8 5 4 8 6 0 9 0 4 0 5 0 4 9 6 6 5\n",
      " 1 3 3 8 7 8 4 9 3 0 0 6 0 0 2 3 9 5 7 8 9 0 5 1 3 1 9 5 9 0 4 2 6 4 1 2 9\n",
      " 9 3 7 7 8 7 4 1 0 6 9 9 6 7 4 6 3 8 3 0 6 0 7 3 2 9 4 8 2 9 2 5 6 9 2 2 4\n",
      " 7 7 7 4 9 5 8 1 7 4 1 5 4 9 0 6 3 3 2 2 8 9 0 9 1 3 7 6 5 2 7 7 8 6 2 2 6\n",
      " 1 9 3 9 8 0 7 5 2 9 5 6 0 5 0 9 8 6 9 2 2 6 0 3 1 3 3 1 9 6 7 7 5 9 5 7 4\n",
      " 8 4 6 7 1 6 2 6 2 1 7 5 4 0 2 4 7 8 3 0 5 8 2 8 8 6 4 7 5 0 9 3 5 0 7 2 4\n",
      " 6 5 2 7 7 6 9 6 4 9 1 3 1 4 2 3 4 1 5 1 7 7 3 0 4 4 8 7 2 8 8 4 7 3 4 0 3\n",
      " 1 6 5 7 3 4 9 6 6 0 8 9 3 5 5 8 7 5 9 9 6 7 6 8 2 0 7 2 0 1 5 1 0 8 2 1 9\n",
      " 7 6 7 1 9 5 4 1 3 7 0 4 5 0 8 3 4 4 0 1 1 1 3 7 4 5 2 1 0 8 2 2 4 0 4 1 4\n",
      " 8 1 7 1 6 3 7 1 3 4 1 6 8 3 4 9 1 8 2 7 4 3 8 4 7 7 6 9 6 9 0 3 9 0 5 7 8\n",
      " 6 1 8 2 1 3 0 5 9 4 0 8 2 1 3 1 4 7 1 6 0 2 7 9 5 9 7 3 5 8 3 2 8 9 7 6 7\n",
      " 8 6 1 2 8 4 1 8 7 4 9 0 8 4 0 2 6 4 1 1 8 8 2 9 7 2 8 4 3 8 4 5 1 0 2 4 3\n",
      " 0 1 2 3 4 2 4 2 4 3 7 6 2 5 6 0 3 5 1 6 7 3 1 6 3 7 8 1 2 6 6 9 3 4 4 8 1\n",
      " 8 9 2 4 7 5 1 1 2 4 0 1 3 8 4 9 5 6 6 1 2 8 3 1 6 2 2 4 8 5 5 4 6 3 7 8 1\n",
      " 7 7 3 6 5 2 9 3 5 6 0 9 2 2 2 8 4 4 0 3 8 6 1 2 4 3 2 9 1 7 6 0 8 1 5 1 8\n",
      " 0 0 6 0 2 7 6 4 2 3 8 4 9 7 1 9 1 3 5 5 8 4 4 3 5 6 0 0 6 2 3 6 1 2 8 3 2\n",
      " 2 6 8 7 8 5 3 3 7 0 6 0 4 5 6 1 1 2 6 7 9 6 2 4 8 9 4 3 4 5 7 9 9 8 3 4 7\n",
      " 8]\n",
      "Pr[H_i|x_-i], j-th row is for j-th sample and i=positions[j]:\n",
      "[[0.4858475  0.37792806 0.13622444]\n",
      " [0.26856986 0.59572166 0.13570848]\n",
      " [0.49690882 0.37645601 0.12663517]\n",
      " ...\n",
      " [0.6169551  0.29593169 0.08711321]\n",
      " [0.27449065 0.49157009 0.23393927]\n",
      " [0.56377752 0.33707864 0.09914384]]\n",
      "Pr[X_i|x_-i], j-th row is for j-th sample and i=positions[j]:\n",
      "[[0.1964585  0.5003722  0.01885767 0.22097542 0.06333621]\n",
      " [0.16752922 0.58897688 0.01955381 0.13050516 0.09343493]\n",
      " [0.19929596 0.49311862 0.01854038 0.22578878 0.06325626]\n",
      " ...\n",
      " [0.22092912 0.43280943 0.01698896 0.27663338 0.05263912]\n",
      " [0.15437796 0.61457871 0.02241373 0.13084951 0.07778008]\n",
      " [0.21212344 0.45796273 0.01751567 0.25422872 0.05816945]]\n",
      "Pr[H_i|x_i,x_-i], j-th row is for j-th sample and i=positions[j]:\n",
      "[[0.24419683 0.49847084 0.25733233]\n",
      " [0.1146812  0.66752714 0.21779166]\n",
      " [0.25343029 0.50383301 0.2427367 ]\n",
      " ...\n",
      " [0.79426245 0.20218326 0.00355429]\n",
      " [0.11232676 0.52787577 0.35979747]\n",
      " [0.75593165 0.23985528 0.00421307]]\n"
     ]
    }
   ],
   "source": [
    "states, obs = generate_HMM_sequences(trans_mat, obs_mat, stat_dist, length, num_samples) # generate sample sequences\n",
    "\n",
    "pos = np.random.randint(length, size = num_samples)\n",
    "\n",
    "print(\"transition matrix\")\n",
    "print(trans_mat)\n",
    "print(\"observation matrix\")\n",
    "print(obs_mat)\n",
    "print(\"stationary distribution\")\n",
    "print(stat_dist)\n",
    "print(\"states and observations, first half of each row is states\")\n",
    "print(np.concatenate((states, obs), axis = 1))\n",
    "print(\"positions: \", pos)\n",
    "h, x, hh = x_i_conditional_prob(trans_mat, obs_mat, stat_dist, obs, pos)\n",
    "print(\"Pr[H_i|x_-i], j-th row is for j-th sample and i=positions[j]:\")\n",
    "print(h)\n",
    "print(\"Pr[X_i|x_-i], j-th row is for j-th sample and i=positions[j]:\")\n",
    "print(x)\n",
    "print(\"Pr[H_i|x_i,x_-i], j-th row is for j-th sample and i=positions[j]:\")\n",
    "print(hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "logh = np.log(h)\n",
    "x_one_hot = np.zeros((num_samples, num_obs))\n",
    "for i in range(num_samples):\n",
    "    x_one_hot[i, int(obs[i, pos[i]])] = 1\n",
    "features1, features2, labels = logh, x_one_hot, hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = data.TensorDataset(torch.FloatTensor(features1), torch.FloatTensor(features2), torch.FloatTensor(labels))\n",
    "test_dl = data.DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8.6408e-02, 5.6694e-01, 3.4665e-01],\n",
      "        [5.9854e-02, 9.4015e-01, 1.3646e-09],\n",
      "        [3.4539e-01, 4.7921e-01, 1.7540e-01],\n",
      "        [1.0109e-01, 6.7997e-01, 2.1894e-01],\n",
      "        [3.8309e-01, 5.3579e-01, 8.1121e-02],\n",
      "        [6.9207e-01, 3.0419e-01, 3.7369e-03],\n",
      "        [6.4259e-01, 3.4938e-01, 8.0260e-03],\n",
      "        [4.9077e-01, 4.9475e-01, 1.4478e-02],\n",
      "        [5.1444e-02, 6.0975e-01, 3.3881e-01]], grad_fn=<SliceBackward>)\n",
      "tensor([[8.6408e-02, 5.6694e-01, 3.4665e-01],\n",
      "        [5.9853e-02, 9.4015e-01, 1.3646e-09],\n",
      "        [3.4539e-01, 4.7921e-01, 1.7540e-01],\n",
      "        [1.0109e-01, 6.7997e-01, 2.1894e-01],\n",
      "        [3.8309e-01, 5.3579e-01, 8.1121e-02],\n",
      "        [6.9207e-01, 3.0419e-01, 3.7369e-03],\n",
      "        [6.4259e-01, 3.4938e-01, 8.0260e-03],\n",
      "        [4.9077e-01, 4.9475e-01, 1.4478e-02],\n",
      "        [5.1444e-02, 6.0975e-01, 3.3881e-01]])\n"
     ]
    }
   ],
   "source": [
    "for X1, X2, y in test_dl:\n",
    "    print(net(X1, X2)[:9])\n",
    "    print(y[:9])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
