{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define some useful functions\n",
    "def generate_HMM_params(num_hidden_state, num_obs):\n",
    "    # random generate the transition matrix and observation matrix, and compute the stationary distribution\n",
    "    \n",
    "    alpha_state = np.ones(num_hidden_state)\n",
    "    alpha_obs = np.ones(num_obs) / num_obs\n",
    "    trans_mat = np.random.dirichlet(alpha_state, num_hidden_state)\n",
    "    obs_mat = np.random.dirichlet(alpha_obs, num_hidden_state)\n",
    "    tmp = np.ones((num_hidden_state + 1, num_hidden_state))\n",
    "    tmp[:-1] = np.identity(num_hidden_state) - trans_mat.T\n",
    "    tmp_v = np.zeros(num_hidden_state + 1)\n",
    "    tmp_v[-1] = 1\n",
    "    stat_dist = np.linalg.lstsq(tmp, tmp_v, rcond=None)[0]\n",
    "    return trans_mat, obs_mat, stat_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_HMM_sequences(trans_mat, obs_mat, init_dist, length, num_samples = 1):\n",
    "    # generate sample sequences from HMM\n",
    "    \n",
    "    states = np.zeros((num_samples, length))\n",
    "    obs = np.zeros((num_samples, length))\n",
    "    tmp_state = np.argmax(np.random.multinomial(1, init_dist, num_samples), axis = 1)\n",
    "    #print(tmp_state)\n",
    "    for i in range(length):\n",
    "        #print(\"i: \", i)\n",
    "        states[:, i] = tmp_state\n",
    "        for j in range(num_samples):\n",
    "            obs[j, i] = np.random.multinomial(1, obs_mat[tmp_state[j]]).argmax()\n",
    "            tmp_state[j] = np.random.multinomial(1, trans_mat[tmp_state[j]]).argmax()\n",
    "        #print(\"obs[:, i]: \", obs[:, i])\n",
    "    return states, obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_compute(trans_mat, obs_mat, init_dist, obs_to_pos):\n",
    "    # compute \\sum_{h_1,...,h_{pos-1}} P(h_1,...,h_{pos},x_1,...,x_{pos-1})\n",
    "    pos = obs_to_pos.shape[0] + 1\n",
    "    num_hidden_state = trans_mat.shape[0]\n",
    "    num_obs = obs_mat.shape[1]\n",
    "    forward = np.zeros((pos, num_hidden_state))\n",
    "    forward[0] = init_dist\n",
    "    for i in range(1, pos):\n",
    "        for j in range(num_hidden_state):\n",
    "            for k in range(num_hidden_state):\n",
    "                #print(i, j, k)\n",
    "                #print(forward[i - 1, k], trans_mat[k, j], obs_mat[k, int(obs_to_pos[i - 1])])\n",
    "                forward[i, j] += forward[i - 1, k] * trans_mat[k, j] * obs_mat[k, int(obs_to_pos[i - 1])]\n",
    "    #print(\"forward: \", forward)\n",
    "    return forward[pos - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_compute(trans_mat, obs_mat, obs_from_pos):\n",
    "    num_hidden_state = trans_mat.shape[0]\n",
    "    num_obs = obs_mat.shape[1]\n",
    "    back_length = obs_from_pos.shape[0]\n",
    "    if (back_length == 0):\n",
    "        return np.ones(num_hidden_state)\n",
    "    backward = np.zeros((back_length, num_hidden_state))\n",
    "    for j in range(num_hidden_state):\n",
    "         for k in range(num_hidden_state):\n",
    "            backward[0, j] += trans_mat[j, k] * obs_mat[k, int(obs_from_pos[-1])]\n",
    "    for i in range(1, back_length):\n",
    "        for j in range(num_hidden_state):\n",
    "            for k in range(num_hidden_state):\n",
    "                backward[i, j] += trans_mat[j, k] * obs_mat[k, int(obs_from_pos[-(i + 1)])] * backward[i - 1, k]\n",
    "    #print(\"backward: \", backward)\n",
    "    return backward[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_i_conditional_prob(trans_mat, obs_mat, init_dist, known_X, pos):\n",
    "    num_hidden_state = trans_mat.shape[0]\n",
    "    num_obs = obs_mat.shape[1]\n",
    "    num_samples = known_X.shape[0]\n",
    "    length = known_X.shape[1]\n",
    "    x_pos_conditional_prob = np.zeros((num_samples, num_obs))\n",
    "    h_pos_conditional_prob = np.zeros((num_samples, num_hidden_state))\n",
    "    h_all_pos_conditional_prob = np.zeros((num_samples, num_hidden_state))\n",
    "    for i in range(num_samples):\n",
    "        #print(\"x_i_conditional_prob: i=\", i)\n",
    "        sample_obs_vec = known_X[i]\n",
    "        forward_vec = forward_compute(trans_mat, obs_mat, init_dist, known_X[i, :pos[i]])\n",
    "        backward_vec = backward_compute(trans_mat, obs_mat, known_X[i, pos[i] + 1:])\n",
    "        #print(\"forward_vec: \", forward_vec)\n",
    "        #print(\"backward_vec: \", backward_vec)\n",
    "        h_prob_tmp = forward_vec * backward_vec\n",
    "        tmp = h_prob_tmp.sum()\n",
    "        h_prob_tmp /= tmp\n",
    "        h_pos_conditional_prob[i] = h_prob_tmp\n",
    "        x_pos_conditional_prob[i] = h_prob_tmp @ obs_mat\n",
    "        h_all_pos_conditional_prob[i] = h_prob_tmp * obs_mat[:, int(known_X[i, pos[i]])] / x_pos_conditional_prob[i, int(known_X[i, pos[i]])]\n",
    "    return h_pos_conditional_prob, x_pos_conditional_prob, h_all_pos_conditional_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameter setting\n",
    "num_hidden_state = 3\n",
    "num_obs = 5\n",
    "length = 10\n",
    "num_samples = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transition matrix\n",
      "[[0.19038151 0.5603291  0.24928939]\n",
      " [0.59211519 0.37810341 0.0297814 ]\n",
      " [0.52737761 0.34205379 0.1305686 ]]\n",
      "observation matrix\n",
      "[[2.84422161e-01 2.51497236e-01 1.31816210e-02 4.38026768e-01\n",
      "  1.28722143e-02]\n",
      " [1.50940815e-01 6.59969385e-01 1.64551320e-02 2.15946003e-02\n",
      "  1.51040067e-01]\n",
      " [9.01408117e-03 9.45219115e-01 4.57668030e-02 7.87716140e-11\n",
      "  4.61558184e-10]]\n",
      "stationary distribution\n",
      "[0.41619456 0.44908825 0.1347172 ]\n",
      "states and observations, first half of each row is states, only showing first 5: \n",
      "[[1. 1. 0. 0. 0. 2. 0. 1. 0. 1. 0. 1. 0. 3. 3. 1. 1. 0. 3. 1.]\n",
      " [1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 4. 3. 3. 4. 1. 0. 4. 1. 1.]\n",
      " [0. 2. 0. 2. 0. 2. 2. 2. 1. 0. 3. 1. 1. 1. 0. 1. 1. 1. 1. 1.]\n",
      " [0. 2. 2. 0. 1. 0. 2. 1. 2. 0. 1. 1. 1. 3. 1. 0. 1. 1. 1. 2.]\n",
      " [1. 0. 1. 0. 2. 1. 1. 1. 0. 1. 4. 1. 4. 1. 1. 0. 1. 0. 3. 3.]]\n",
      "positions, only showing first 5:  [9 9 9 9 9]\n",
      "Pr[H_i|x_-i], j-th row is for j-th sample and i=positions[j], only showing first 5:\n",
      "[[0.21892801 0.54738046 0.23369153]\n",
      " [0.45485649 0.43452393 0.11061958]\n",
      " [0.47997322 0.41608534 0.10394145]\n",
      " [0.48386551 0.41376682 0.10236767]\n",
      " [0.22070869 0.54657275 0.23271857]]\n",
      "Pr[X_i|x_-i], j-th row is for j-th sample and i=positions[j], only showing first 5:\n",
      "[[0.14699655 0.63720383 0.02258836 0.10771679 0.08549447]\n",
      " [0.1959558  0.50572738 0.0182086  0.20862269 0.07148553]\n",
      " [0.20025622 0.49356296 0.01793063 0.21922631 0.06902388]\n",
      " [0.20099913 0.49152415 0.01787176 0.22088117 0.06872379]\n",
      " [0.14737232 0.63619894 0.02255401 0.10847933 0.08539539]]\n",
      "Pr[H_i|x_i,x_-i], j-th row is for j-th sample and i=positions[j], only showing first 5:\n",
      "[[8.64084416e-02 5.66936866e-01 3.46654692e-01]\n",
      " [2.26199243e-01 5.67049562e-01 2.06751195e-01]\n",
      " [2.44572519e-01 5.56369915e-01 1.99057566e-01]\n",
      " [3.56883238e-01 3.80969056e-01 2.62147706e-01]\n",
      " [8.91195680e-01 1.08804320e-01 1.68987184e-10]]\n"
     ]
    }
   ],
   "source": [
    "seed = 20211018\n",
    "np.random.seed(seed)\n",
    "trans_mat, obs_mat, stat_dist = generate_HMM_params(num_hidden_state, num_obs) # generate parameters for HMM\n",
    "\n",
    "states, obs = generate_HMM_sequences(trans_mat, obs_mat, stat_dist, length, num_samples) # generate sample sequences\n",
    "\n",
    "pos = np.ones(num_samples).astype(int) * (length - 1)\n",
    "\n",
    "print(\"transition matrix\")\n",
    "print(trans_mat)\n",
    "print(\"observation matrix\")\n",
    "print(obs_mat)\n",
    "print(\"stationary distribution\")\n",
    "print(stat_dist)\n",
    "print(\"states and observations, first half of each row is states, only showing first 5: \")\n",
    "print(np.concatenate((states, obs), axis = 1)[:5])\n",
    "print(\"positions, only showing first 5: \", pos[:5])\n",
    "h, x, hh = x_i_conditional_prob(trans_mat, obs_mat, stat_dist, obs, pos)\n",
    "print(\"Pr[H_i|x_-i], j-th row is for j-th sample and i=positions[j], only showing first 5:\")\n",
    "print(h[:5])\n",
    "print(\"Pr[X_i|x_-i], j-th row is for j-th sample and i=positions[j], only showing first 5:\")\n",
    "print(x[:5])\n",
    "print(\"Pr[H_i|x_i,x_-i], j-th row is for j-th sample and i=positions[j], only showing first 5:\")\n",
    "print(hh[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = obs.astype('long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, ..., 1, 0, 3],\n",
       "       [1, 4, 3, ..., 0, 4, 1],\n",
       "       [3, 1, 1, ..., 1, 1, 1],\n",
       "       ...,\n",
       "       [2, 3, 1, ..., 1, 0, 1],\n",
       "       [1, 1, 3, ..., 1, 1, 1],\n",
       "       [3, 3, 3, ..., 1, 1, 1]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_obs, labels = obs[:, :-1], obs[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.zeros((num_samples, length, num_obs)) # last input will always be 0 since we want to predict the last x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_samples):\n",
    "    for j in range(length - 1):\n",
    "        features[i, j, pre_obs[i, j]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.in2hidden = nn.Sequential(\n",
    "            nn.Linear(num_obs + hidden_size, hidden_size)\n",
    "        )\n",
    "        self.in2output = nn.Sequential(\n",
    "            nn.Linear(num_obs + hidden_size, num_obs)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, hidden_state):\n",
    "        combined = torch.cat((x, hidden_state), 1)\n",
    "        hidden = torch.sigmoid(self.in2hidden(combined))\n",
    "        output = self.in2output(combined)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return nn.init.kaiming_uniform_(torch.empty(batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters.\n",
    "lr = 0.01\n",
    "epochs = 1000\n",
    "batch_size = 100\n",
    "hidden_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.TensorDataset(torch.FloatTensor(features), torch.LongTensor(labels))\n",
    "train_dl = data.DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SimpleRNN(hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "loss:  tensor(14.7232, grad_fn=<AddBackward0>)\n",
      "epoch:  10\n",
      "loss:  tensor(12.3436, grad_fn=<AddBackward0>)\n",
      "epoch:  20\n",
      "loss:  tensor(12.3007, grad_fn=<AddBackward0>)\n",
      "epoch:  30\n",
      "loss:  tensor(12.2482, grad_fn=<AddBackward0>)\n",
      "epoch:  40\n",
      "loss:  tensor(12.0958, grad_fn=<AddBackward0>)\n",
      "epoch:  50\n",
      "loss:  tensor(11.6210, grad_fn=<AddBackward0>)\n",
      "epoch:  60\n",
      "loss:  tensor(10.1881, grad_fn=<AddBackward0>)\n",
      "epoch:  70\n",
      "loss:  tensor(7.6043, grad_fn=<AddBackward0>)\n",
      "epoch:  80\n",
      "loss:  tensor(4.2316, grad_fn=<AddBackward0>)\n",
      "epoch:  90\n",
      "loss:  tensor(2.5895, grad_fn=<AddBackward0>)\n",
      "epoch:  100\n",
      "loss:  tensor(1.2167, grad_fn=<AddBackward0>)\n",
      "epoch:  110\n",
      "loss:  tensor(0.9326, grad_fn=<AddBackward0>)\n",
      "epoch:  120\n",
      "loss:  tensor(0.9204, grad_fn=<AddBackward0>)\n",
      "epoch:  130\n",
      "loss:  tensor(0.9161, grad_fn=<AddBackward0>)\n",
      "epoch:  140\n",
      "loss:  tensor(0.7564, grad_fn=<AddBackward0>)\n",
      "epoch:  150\n",
      "loss:  tensor(0.5659, grad_fn=<AddBackward0>)\n",
      "epoch:  160\n",
      "loss:  tensor(0.5376, grad_fn=<AddBackward0>)\n",
      "epoch:  170\n",
      "loss:  tensor(0.5293, grad_fn=<AddBackward0>)\n",
      "epoch:  180\n",
      "loss:  tensor(0.5190, grad_fn=<AddBackward0>)\n",
      "epoch:  190\n",
      "loss:  tensor(0.5118, grad_fn=<AddBackward0>)\n",
      "epoch:  200\n",
      "loss:  tensor(0.5026, grad_fn=<AddBackward0>)\n",
      "epoch:  210\n",
      "loss:  tensor(0.5008, grad_fn=<AddBackward0>)\n",
      "epoch:  220\n",
      "loss:  tensor(0.4973, grad_fn=<AddBackward0>)\n",
      "epoch:  230\n",
      "loss:  tensor(1.3956, grad_fn=<AddBackward0>)\n",
      "epoch:  240\n",
      "loss:  tensor(0.5675, grad_fn=<AddBackward0>)\n",
      "epoch:  250\n",
      "loss:  tensor(0.5477, grad_fn=<AddBackward0>)\n",
      "epoch:  260\n",
      "loss:  tensor(0.4985, grad_fn=<AddBackward0>)\n",
      "epoch:  270\n",
      "loss:  tensor(0.4976, grad_fn=<AddBackward0>)\n",
      "epoch:  280\n",
      "loss:  tensor(0.4872, grad_fn=<AddBackward0>)\n",
      "epoch:  290\n",
      "loss:  tensor(0.4851, grad_fn=<AddBackward0>)\n",
      "epoch:  300\n",
      "loss:  tensor(1.3315, grad_fn=<AddBackward0>)\n",
      "epoch:  310\n",
      "loss:  tensor(0.7076, grad_fn=<AddBackward0>)\n",
      "epoch:  320\n",
      "loss:  tensor(0.6188, grad_fn=<AddBackward0>)\n",
      "epoch:  330\n",
      "loss:  tensor(0.4981, grad_fn=<AddBackward0>)\n",
      "epoch:  340\n",
      "loss:  tensor(0.4836, grad_fn=<AddBackward0>)\n",
      "epoch:  350\n",
      "loss:  tensor(0.5100, grad_fn=<AddBackward0>)\n",
      "epoch:  360\n",
      "loss:  tensor(0.4866, grad_fn=<AddBackward0>)\n",
      "epoch:  370\n",
      "loss:  tensor(0.4783, grad_fn=<AddBackward0>)\n",
      "epoch:  380\n",
      "loss:  tensor(0.7722, grad_fn=<AddBackward0>)\n",
      "epoch:  390\n",
      "loss:  tensor(0.4958, grad_fn=<AddBackward0>)\n",
      "epoch:  400\n",
      "loss:  tensor(0.4853, grad_fn=<AddBackward0>)\n",
      "epoch:  410\n",
      "loss:  tensor(0.4791, grad_fn=<AddBackward0>)\n",
      "epoch:  420\n",
      "loss:  tensor(0.4817, grad_fn=<AddBackward0>)\n",
      "epoch:  430\n",
      "loss:  tensor(0.5550, grad_fn=<AddBackward0>)\n",
      "epoch:  440\n",
      "loss:  tensor(0.4798, grad_fn=<AddBackward0>)\n",
      "epoch:  450\n",
      "loss:  tensor(0.4755, grad_fn=<AddBackward0>)\n",
      "epoch:  460\n",
      "loss:  tensor(0.4721, grad_fn=<AddBackward0>)\n",
      "epoch:  470\n",
      "loss:  tensor(0.4746, grad_fn=<AddBackward0>)\n",
      "epoch:  480\n",
      "loss:  tensor(0.4706, grad_fn=<AddBackward0>)\n",
      "epoch:  490\n",
      "loss:  tensor(0.4716, grad_fn=<AddBackward0>)\n",
      "epoch:  500\n",
      "loss:  tensor(0.4693, grad_fn=<AddBackward0>)\n",
      "epoch:  510\n",
      "loss:  tensor(0.4703, grad_fn=<AddBackward0>)\n",
      "epoch:  520\n",
      "loss:  tensor(0.4682, grad_fn=<AddBackward0>)\n",
      "epoch:  530\n",
      "loss:  tensor(0.4698, grad_fn=<AddBackward0>)\n",
      "epoch:  540\n",
      "loss:  tensor(0.4700, grad_fn=<AddBackward0>)\n",
      "epoch:  550\n",
      "loss:  tensor(0.4681, grad_fn=<AddBackward0>)\n",
      "epoch:  560\n",
      "loss:  tensor(0.4682, grad_fn=<AddBackward0>)\n",
      "epoch:  570\n",
      "loss:  tensor(0.4670, grad_fn=<AddBackward0>)\n",
      "epoch:  580\n",
      "loss:  tensor(0.4665, grad_fn=<AddBackward0>)\n",
      "epoch:  590\n",
      "loss:  tensor(0.4667, grad_fn=<AddBackward0>)\n",
      "epoch:  600\n",
      "loss:  tensor(0.4672, grad_fn=<AddBackward0>)\n",
      "epoch:  610\n",
      "loss:  tensor(0.4668, grad_fn=<AddBackward0>)\n",
      "epoch:  620\n",
      "loss:  tensor(0.4669, grad_fn=<AddBackward0>)\n",
      "epoch:  630\n",
      "loss:  tensor(0.4679, grad_fn=<AddBackward0>)\n",
      "epoch:  640\n",
      "loss:  tensor(0.4656, grad_fn=<AddBackward0>)\n",
      "epoch:  650\n",
      "loss:  tensor(0.4656, grad_fn=<AddBackward0>)\n",
      "epoch:  660\n",
      "loss:  tensor(0.4651, grad_fn=<AddBackward0>)\n",
      "epoch:  670\n",
      "loss:  tensor(3.6152, grad_fn=<AddBackward0>)\n",
      "epoch:  680\n",
      "loss:  tensor(0.5644, grad_fn=<AddBackward0>)\n",
      "epoch:  690\n",
      "loss:  tensor(0.5566, grad_fn=<AddBackward0>)\n",
      "epoch:  700\n",
      "loss:  tensor(0.4986, grad_fn=<AddBackward0>)\n",
      "epoch:  710\n",
      "loss:  tensor(0.4869, grad_fn=<AddBackward0>)\n",
      "epoch:  720\n",
      "loss:  tensor(0.4836, grad_fn=<AddBackward0>)\n",
      "epoch:  730\n",
      "loss:  tensor(0.5055, grad_fn=<AddBackward0>)\n",
      "epoch:  740\n",
      "loss:  tensor(0.4736, grad_fn=<AddBackward0>)\n",
      "epoch:  750\n",
      "loss:  tensor(0.4782, grad_fn=<AddBackward0>)\n",
      "epoch:  760\n",
      "loss:  tensor(0.4728, grad_fn=<AddBackward0>)\n",
      "epoch:  770\n",
      "loss:  tensor(0.4828, grad_fn=<AddBackward0>)\n",
      "epoch:  780\n",
      "loss:  tensor(0.4762, grad_fn=<AddBackward0>)\n",
      "epoch:  790\n",
      "loss:  tensor(0.4745, grad_fn=<AddBackward0>)\n",
      "epoch:  800\n",
      "loss:  tensor(0.7110, grad_fn=<AddBackward0>)\n",
      "epoch:  810\n",
      "loss:  tensor(0.5261, grad_fn=<AddBackward0>)\n",
      "epoch:  820\n",
      "loss:  tensor(0.5992, grad_fn=<AddBackward0>)\n",
      "epoch:  830\n",
      "loss:  tensor(0.9636, grad_fn=<AddBackward0>)\n",
      "epoch:  840\n",
      "loss:  tensor(0.4872, grad_fn=<AddBackward0>)\n",
      "epoch:  850\n",
      "loss:  tensor(0.5103, grad_fn=<AddBackward0>)\n",
      "epoch:  860\n",
      "loss:  tensor(0.4775, grad_fn=<AddBackward0>)\n",
      "epoch:  870\n",
      "loss:  tensor(0.4699, grad_fn=<AddBackward0>)\n",
      "epoch:  880\n",
      "loss:  tensor(0.4716, grad_fn=<AddBackward0>)\n",
      "epoch:  890\n",
      "loss:  tensor(0.5916, grad_fn=<AddBackward0>)\n",
      "epoch:  900\n",
      "loss:  tensor(1.1255, grad_fn=<AddBackward0>)\n",
      "epoch:  910\n",
      "loss:  tensor(0.5105, grad_fn=<AddBackward0>)\n",
      "epoch:  920\n",
      "loss:  tensor(0.4819, grad_fn=<AddBackward0>)\n",
      "epoch:  930\n",
      "loss:  tensor(0.4678, grad_fn=<AddBackward0>)\n",
      "epoch:  940\n",
      "loss:  tensor(0.4732, grad_fn=<AddBackward0>)\n",
      "epoch:  950\n",
      "loss:  tensor(0.4917, grad_fn=<AddBackward0>)\n",
      "epoch:  960\n",
      "loss:  tensor(0.4682, grad_fn=<AddBackward0>)\n",
      "epoch:  970\n",
      "loss:  tensor(0.4663, grad_fn=<AddBackward0>)\n",
      "epoch:  980\n",
      "loss:  tensor(0.4934, grad_fn=<AddBackward0>)\n",
      "epoch:  990\n",
      "loss:  tensor(0.4916, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "total_loss_lst = []\n",
    "for i in range(epochs):\n",
    "    total_loss = 0\n",
    "    for X, Y in train_dl:\n",
    "        hidden_state = net.init_hidden()\n",
    "        for j in range(length):\n",
    "            output, hidden_state = net(X[:, j], hidden_state)\n",
    "        l = loss(output, Y)\n",
    "        total_loss += l\n",
    "        trainer.zero_grad()\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "    if (i % 10 == 0):\n",
    "        print(\"epoch: \", i)\n",
    "        print(\"loss: \", total_loss)\n",
    "        total_loss_lst.append(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x_lst = list(range(0, 1000, 10))\n",
    "plt.plot(x_lst, total_loss_lst)\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"training loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleRNN(\n",
       "  (in2hidden): Sequential(\n",
       "    (0): Linear(in_features=261, out_features=256, bias=True)\n",
       "  )\n",
       "  (in2output): Sequential(\n",
       "    (0): Linear(in_features=261, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -2.4785,  10.6795, -29.7537, -13.4525,  -8.9671],\n",
      "        [ -2.8479,   9.2362,  -3.9544, -15.7425,  -2.3572],\n",
      "        [ -4.4779,   8.1171, -30.8979,  -1.5834, -10.5022],\n",
      "        [ -8.5444,   2.4791,   8.1376,  -5.5507,   0.5613],\n",
      "        [-10.9735,   0.1292, -25.4456,   9.8268,  -9.6339],\n",
      "        [-10.4013,  12.6164, -12.2187,  -7.8498, -11.3978],\n",
      "        [ -1.6965,  -0.3629, -20.5261,   7.5045,  -8.4734],\n",
      "        [ -3.8124,  -4.8164,  -9.6353,   6.0021,  -2.3333],\n",
      "        [ -5.2684,   4.2564,  -7.9998,  -2.9044,  -5.1460],\n",
      "        [ -1.2091,  -3.8901, -34.6399,  12.5897, -14.0428]],\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([[1.9299e-06, 1.0000e+00, 2.7546e-18, 3.3082e-11, 2.9348e-09],\n",
      "        [5.6485e-06, 9.9998e-01, 1.8680e-06, 1.4186e-11, 9.2261e-06],\n",
      "        [3.3886e-06, 9.9994e-01, 1.1375e-17, 6.1249e-05, 8.1981e-09],\n",
      "        [5.6670e-08, 3.4738e-03, 9.9601e-01, 1.1311e-06, 5.1042e-04],\n",
      "        [9.2578e-10, 6.1425e-05, 4.8011e-16, 9.9994e-01, 3.5342e-09],\n",
      "        [1.0082e-10, 1.0000e+00, 1.6378e-11, 1.2932e-09, 3.7223e-11],\n",
      "        [1.0088e-04, 3.8283e-04, 6.7024e-13, 9.9952e-01, 1.1499e-07],\n",
      "        [5.4635e-05, 2.0020e-05, 1.6166e-07, 9.9969e-01, 2.3981e-04],\n",
      "        [7.2949e-05, 9.9906e-01, 4.7509e-06, 7.7567e-04, 8.2444e-05],\n",
      "        [1.0169e-06, 6.9654e-08, 3.0792e-21, 1.0000e+00, 2.7145e-12]],\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([1, 1, 1, 2, 3, 1, 3, 3, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "for X, Y in train_dl:\n",
    "    hidden_state = net.init_hidden()\n",
    "    for j in range(length):\n",
    "        output, hidden_state = net(X[:, j], hidden_state)\n",
    "    print(output[:10])\n",
    "    m = nn.Softmax(dim = 1)\n",
    "    print(m(output)[:10])\n",
    "    print(Y[:10])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transition matrix:\n",
      "[[0.19038151 0.5603291  0.24928939]\n",
      " [0.59211519 0.37810341 0.0297814 ]\n",
      " [0.52737761 0.34205379 0.1305686 ]]\n",
      "observation matrix:\n",
      "[[2.84422161e-01 2.51497236e-01 1.31816210e-02 4.38026768e-01\n",
      "  1.28722143e-02]\n",
      " [1.50940815e-01 6.59969385e-01 1.64551320e-02 2.15946003e-02\n",
      "  1.51040067e-01]\n",
      " [9.01408117e-03 9.45219115e-01 4.57668030e-02 7.87716140e-11\n",
      "  4.61558184e-10]]\n",
      "stationary distribution:\n",
      "[0.41619456 0.44908825 0.1347172 ]\n",
      "states and observations, first half of each row is states, only showing first 5:\n",
      "[[1. 0. 2. 2. 0. 0. 2. 2. 0. 1. 4. 3. 1. 1. 3. 1. 1. 1. 3. 4.]\n",
      " [2. 1. 0. 0. 2. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 4. 4. 1. 4. 0.]\n",
      " [1. 0. 0. 1. 2. 1. 1. 1. 1. 0. 1. 0. 3. 0. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 3. 1. 1. 3. 1. 1. 0. 1. 3.]\n",
      " [1. 0. 1. 0. 1. 1. 0. 0. 1. 2. 4. 3. 0. 1. 0. 1. 1. 0. 2. 1.]]\n",
      "positions, only showing first 5:  [0 6 9 3 5]\n",
      "Pr[H_i|x_-i], j-th row is for j-th sample and i=positions[j], only showing first 5:\n",
      "[[0.20310978 0.62881966 0.16807056]\n",
      " [0.62262926 0.33106724 0.0463035 ]\n",
      " [0.48084875 0.41557025 0.10358101]\n",
      " [0.2916593  0.59293973 0.11540097]\n",
      " [0.42694635 0.40522244 0.16783121]]\n",
      "Pr[X_i|x_-i], j-th row is for j-th sample and i=positions[j], only showing first 5:\n",
      "[[0.15419848 0.62494678 0.02071668 0.10254663 0.09759144]\n",
      " [0.2274785  0.41885073 0.01577418 0.27987755 0.05801903]\n",
      " [0.20042424 0.49310252 0.0179172  0.2195987  0.06895735]\n",
      " [0.17349341 0.57375278 0.01888298 0.14055888 0.09331196]\n",
      " [0.18411045 0.5334475  0.01997693 0.19576455 0.06670057]]\n",
      "Pr[H_i|x_i,x_-i], j-th row is for j-th sample and i=positions[j], only showing first 5:\n",
      "[[2.67899806e-02 9.73210019e-01 7.94888826e-10]\n",
      " [1.38137722e-01 8.61862278e-01 3.68357749e-10]\n",
      " [2.45247441e-01 5.56200042e-01 1.98552517e-01]\n",
      " [1.27845144e-01 6.82039516e-01 1.90115340e-01]\n",
      " [2.01286586e-01 5.01332191e-01 2.97381223e-01]]\n"
     ]
    }
   ],
   "source": [
    "states, obs = generate_HMM_sequences(trans_mat, obs_mat, stat_dist, length, num_samples) # generate sample sequences\n",
    "\n",
    "pos = np.random.randint(length, size = num_samples)\n",
    "\n",
    "print(\"transition matrix:\")\n",
    "print(trans_mat)\n",
    "print(\"observation matrix:\")\n",
    "print(obs_mat)\n",
    "print(\"stationary distribution:\")\n",
    "print(stat_dist)\n",
    "print(\"states and observations, first half of each row is states, only showing first 5:\")\n",
    "print(np.concatenate((states, obs), axis = 1)[:5])\n",
    "print(\"positions, only showing first 5: \", pos[:5])\n",
    "h, x, hh = x_i_conditional_prob(trans_mat, obs_mat, stat_dist, obs, pos)\n",
    "print(\"Pr[H_i|x_-i], j-th row is for j-th sample and i=positions[j], only showing first 5:\")\n",
    "print(h[:5])\n",
    "print(\"Pr[X_i|x_-i], j-th row is for j-th sample and i=positions[j], only showing first 5:\")\n",
    "print(x[:5])\n",
    "print(\"Pr[H_i|x_i,x_-i], j-th row is for j-th sample and i=positions[j], only showing first 5:\")\n",
    "print(hh[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = obs.astype('long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_obs, labels = obs[:, :-1], obs[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.zeros((num_samples, length, num_obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_samples):\n",
    "    for j in range(length - 1):\n",
    "        features[i, j, pre_obs[i, j]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = data.TensorDataset(torch.FloatTensor(features), torch.LongTensor(labels))\n",
    "test_dl = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(54.8504, grad_fn=<AddBackward0>)\n",
      "tensor(0.3930)\n"
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "num_correct = 0\n",
    "for X, Y in test_dl:\n",
    "    hidden_state = net.init_hidden()\n",
    "    for j in range(length):\n",
    "        output, hidden_state = net(X[:, j], hidden_state)\n",
    "    l = loss(output, Y)\n",
    "    total_loss += l\n",
    "    #m = nn.Softmax(dim = 1)\n",
    "    #print(m(output)[:1])\n",
    "    #print(Y[:1])\n",
    "    tmp = torch.argmax(output, dim = 1)\n",
    "    num_correct += torch.sum((tmp == Y))\n",
    "print(total_loss)\n",
    "print(num_correct.float() / num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
